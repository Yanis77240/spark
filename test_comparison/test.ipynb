{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "All arrays must be of the same length",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/stephan/Documents/github/tdp-alliage-jenkins/spark/test_comparison/test.ipynb Cell 3\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/stephan/Documents/github/tdp-alliage-jenkins/spark/test_comparison/test.ipynb#W3sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mread_json(\u001b[39m'\u001b[39m\u001b[39mresults.json\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m/nix/store/7dnlgfzajj2drspcdhpdxnpxygkak7m5-python3.10-pandas-1.4.3/lib/python3.10/site-packages/pandas/util/_decorators.py:207\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    205\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    206\u001b[0m         kwargs[new_arg_name] \u001b[39m=\u001b[39m new_arg_value\n\u001b[0;32m--> 207\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/nix/store/7dnlgfzajj2drspcdhpdxnpxygkak7m5-python3.10-pandas-1.4.3/lib/python3.10/site-packages/pandas/util/_decorators.py:311\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    305\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(args) \u001b[39m>\u001b[39m num_allow_args:\n\u001b[1;32m    306\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[1;32m    307\u001b[0m         msg\u001b[39m.\u001b[39mformat(arguments\u001b[39m=\u001b[39marguments),\n\u001b[1;32m    308\u001b[0m         \u001b[39mFutureWarning\u001b[39;00m,\n\u001b[1;32m    309\u001b[0m         stacklevel\u001b[39m=\u001b[39mstacklevel,\n\u001b[1;32m    310\u001b[0m     )\n\u001b[0;32m--> 311\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/nix/store/7dnlgfzajj2drspcdhpdxnpxygkak7m5-python3.10-pandas-1.4.3/lib/python3.10/site-packages/pandas/io/json/_json.py:612\u001b[0m, in \u001b[0;36mread_json\u001b[0;34m(path_or_buf, orient, typ, dtype, convert_axes, convert_dates, keep_default_dates, numpy, precise_float, date_unit, encoding, encoding_errors, lines, chunksize, compression, nrows, storage_options)\u001b[0m\n\u001b[1;32m    609\u001b[0m     \u001b[39mreturn\u001b[39;00m json_reader\n\u001b[1;32m    611\u001b[0m \u001b[39mwith\u001b[39;00m json_reader:\n\u001b[0;32m--> 612\u001b[0m     \u001b[39mreturn\u001b[39;00m json_reader\u001b[39m.\u001b[39;49mread()\n",
      "File \u001b[0;32m/nix/store/7dnlgfzajj2drspcdhpdxnpxygkak7m5-python3.10-pandas-1.4.3/lib/python3.10/site-packages/pandas/io/json/_json.py:746\u001b[0m, in \u001b[0;36mJsonReader.read\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    744\u001b[0m         obj \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_object_parser(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_combine_lines(data_lines))\n\u001b[1;32m    745\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 746\u001b[0m     obj \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_object_parser(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdata)\n\u001b[1;32m    747\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclose()\n\u001b[1;32m    748\u001b[0m \u001b[39mreturn\u001b[39;00m obj\n",
      "File \u001b[0;32m/nix/store/7dnlgfzajj2drspcdhpdxnpxygkak7m5-python3.10-pandas-1.4.3/lib/python3.10/site-packages/pandas/io/json/_json.py:768\u001b[0m, in \u001b[0;36mJsonReader._get_object_parser\u001b[0;34m(self, json)\u001b[0m\n\u001b[1;32m    766\u001b[0m obj \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    767\u001b[0m \u001b[39mif\u001b[39;00m typ \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mframe\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m--> 768\u001b[0m     obj \u001b[39m=\u001b[39m FrameParser(json, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\u001b[39m.\u001b[39;49mparse()\n\u001b[1;32m    770\u001b[0m \u001b[39mif\u001b[39;00m typ \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mseries\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mor\u001b[39;00m obj \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    771\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(dtype, \u001b[39mbool\u001b[39m):\n",
      "File \u001b[0;32m/nix/store/7dnlgfzajj2drspcdhpdxnpxygkak7m5-python3.10-pandas-1.4.3/lib/python3.10/site-packages/pandas/io/json/_json.py:880\u001b[0m, in \u001b[0;36mParser.parse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    878\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_parse_numpy()\n\u001b[1;32m    879\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 880\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_parse_no_numpy()\n\u001b[1;32m    882\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobj \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    883\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m/nix/store/7dnlgfzajj2drspcdhpdxnpxygkak7m5-python3.10-pandas-1.4.3/lib/python3.10/site-packages/pandas/io/json/_json.py:1132\u001b[0m, in \u001b[0;36mFrameParser._parse_no_numpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1129\u001b[0m orient \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39morient\n\u001b[1;32m   1131\u001b[0m \u001b[39mif\u001b[39;00m orient \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mcolumns\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m-> 1132\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobj \u001b[39m=\u001b[39m DataFrame(\n\u001b[1;32m   1133\u001b[0m         loads(json, precise_float\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mprecise_float), dtype\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m\n\u001b[1;32m   1134\u001b[0m     )\n\u001b[1;32m   1135\u001b[0m \u001b[39melif\u001b[39;00m orient \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39msplit\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m   1136\u001b[0m     decoded \u001b[39m=\u001b[39m {\n\u001b[1;32m   1137\u001b[0m         \u001b[39mstr\u001b[39m(k): v\n\u001b[1;32m   1138\u001b[0m         \u001b[39mfor\u001b[39;00m k, v \u001b[39min\u001b[39;00m loads(json, precise_float\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprecise_float)\u001b[39m.\u001b[39mitems()\n\u001b[1;32m   1139\u001b[0m     }\n",
      "File \u001b[0;32m/nix/store/7dnlgfzajj2drspcdhpdxnpxygkak7m5-python3.10-pandas-1.4.3/lib/python3.10/site-packages/pandas/core/frame.py:636\u001b[0m, in \u001b[0;36mDataFrame.__init__\u001b[0;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    630\u001b[0m     mgr \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_init_mgr(\n\u001b[1;32m    631\u001b[0m         data, axes\u001b[39m=\u001b[39m{\u001b[39m\"\u001b[39m\u001b[39mindex\u001b[39m\u001b[39m\"\u001b[39m: index, \u001b[39m\"\u001b[39m\u001b[39mcolumns\u001b[39m\u001b[39m\"\u001b[39m: columns}, dtype\u001b[39m=\u001b[39mdtype, copy\u001b[39m=\u001b[39mcopy\n\u001b[1;32m    632\u001b[0m     )\n\u001b[1;32m    634\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(data, \u001b[39mdict\u001b[39m):\n\u001b[1;32m    635\u001b[0m     \u001b[39m# GH#38939 de facto copy defaults to False only in non-dict cases\u001b[39;00m\n\u001b[0;32m--> 636\u001b[0m     mgr \u001b[39m=\u001b[39m dict_to_mgr(data, index, columns, dtype\u001b[39m=\u001b[39;49mdtype, copy\u001b[39m=\u001b[39;49mcopy, typ\u001b[39m=\u001b[39;49mmanager)\n\u001b[1;32m    637\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(data, ma\u001b[39m.\u001b[39mMaskedArray):\n\u001b[1;32m    638\u001b[0m     \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mma\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmrecords\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mmrecords\u001b[39;00m\n",
      "File \u001b[0;32m/nix/store/7dnlgfzajj2drspcdhpdxnpxygkak7m5-python3.10-pandas-1.4.3/lib/python3.10/site-packages/pandas/core/internals/construction.py:502\u001b[0m, in \u001b[0;36mdict_to_mgr\u001b[0;34m(data, index, columns, dtype, typ, copy)\u001b[0m\n\u001b[1;32m    494\u001b[0m     arrays \u001b[39m=\u001b[39m [\n\u001b[1;32m    495\u001b[0m         x\n\u001b[1;32m    496\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mhasattr\u001b[39m(x, \u001b[39m\"\u001b[39m\u001b[39mdtype\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mor\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(x\u001b[39m.\u001b[39mdtype, ExtensionDtype)\n\u001b[1;32m    497\u001b[0m         \u001b[39melse\u001b[39;00m x\u001b[39m.\u001b[39mcopy()\n\u001b[1;32m    498\u001b[0m         \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m arrays\n\u001b[1;32m    499\u001b[0m     ]\n\u001b[1;32m    500\u001b[0m     \u001b[39m# TODO: can we get rid of the dt64tz special case above?\u001b[39;00m\n\u001b[0;32m--> 502\u001b[0m \u001b[39mreturn\u001b[39;00m arrays_to_mgr(arrays, columns, index, dtype\u001b[39m=\u001b[39;49mdtype, typ\u001b[39m=\u001b[39;49mtyp, consolidate\u001b[39m=\u001b[39;49mcopy)\n",
      "File \u001b[0;32m/nix/store/7dnlgfzajj2drspcdhpdxnpxygkak7m5-python3.10-pandas-1.4.3/lib/python3.10/site-packages/pandas/core/internals/construction.py:120\u001b[0m, in \u001b[0;36marrays_to_mgr\u001b[0;34m(arrays, columns, index, dtype, verify_integrity, typ, consolidate)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[39mif\u001b[39;00m verify_integrity:\n\u001b[1;32m    118\u001b[0m     \u001b[39m# figure out the index, if necessary\u001b[39;00m\n\u001b[1;32m    119\u001b[0m     \u001b[39mif\u001b[39;00m index \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 120\u001b[0m         index \u001b[39m=\u001b[39m _extract_index(arrays)\n\u001b[1;32m    121\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    122\u001b[0m         index \u001b[39m=\u001b[39m ensure_index(index)\n",
      "File \u001b[0;32m/nix/store/7dnlgfzajj2drspcdhpdxnpxygkak7m5-python3.10-pandas-1.4.3/lib/python3.10/site-packages/pandas/core/internals/construction.py:674\u001b[0m, in \u001b[0;36m_extract_index\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m    672\u001b[0m lengths \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\u001b[39mset\u001b[39m(raw_lengths))\n\u001b[1;32m    673\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(lengths) \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m--> 674\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mAll arrays must be of the same length\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    676\u001b[0m \u001b[39mif\u001b[39;00m have_dicts:\n\u001b[1;32m    677\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    678\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mMixing dicts with non-Series may lead to ambiguous ordering.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    679\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: All arrays must be of the same length"
     ]
    }
   ],
   "source": [
    "df = pd.read_json('results.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Test_group</th>\n",
       "      <th>Test_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TestReadWriteDiskValidator</td>\n",
       "      <td>testReadWriteDiskValidator</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TestReadWriteDiskValidator</td>\n",
       "      <td>testCheckFailures</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TestReadWriteDiskValidator</td>\n",
       "      <td>testReadWriteDiskValidator</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TestReadWriteDiskValidator</td>\n",
       "      <td>testCheckFailures</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TestReadWriteDiskValidator</td>\n",
       "      <td>testReadWriteDiskValidator</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>TestMRTimelineEventHandling</td>\n",
       "      <td>testMRNewTimelineServiceEventHandling</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>TestMRTimelineEventHandling</td>\n",
       "      <td>testMRNewTimelineServiceEventHandling</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>TestMRTimelineEventHandling</td>\n",
       "      <td>testMRNewTimelineServiceEventHandling</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>TestMRTimelineEventHandling</td>\n",
       "      <td>testMRNewTimelineServiceEventHandling</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>TestSLSRunner</td>\n",
       "      <td>testSimulatorRunning[Testing with: SYNTH, org....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>96 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Test_group  \\\n",
       "0    TestReadWriteDiskValidator   \n",
       "1    TestReadWriteDiskValidator   \n",
       "2    TestReadWriteDiskValidator   \n",
       "3    TestReadWriteDiskValidator   \n",
       "4    TestReadWriteDiskValidator   \n",
       "..                          ...   \n",
       "91  TestMRTimelineEventHandling   \n",
       "92  TestMRTimelineEventHandling   \n",
       "93  TestMRTimelineEventHandling   \n",
       "94  TestMRTimelineEventHandling   \n",
       "95                TestSLSRunner   \n",
       "\n",
       "                                            Test_name  \n",
       "0                          testReadWriteDiskValidator  \n",
       "1                                   testCheckFailures  \n",
       "2                          testReadWriteDiskValidator  \n",
       "3                                   testCheckFailures  \n",
       "4                          testReadWriteDiskValidator  \n",
       "..                                                ...  \n",
       "91              testMRNewTimelineServiceEventHandling  \n",
       "92              testMRNewTimelineServiceEventHandling  \n",
       "93              testMRNewTimelineServiceEventHandling  \n",
       "94              testMRNewTimelineServiceEventHandling  \n",
       "95  testSimulatorRunning[Testing with: SYNTH, org....  \n",
       "\n",
       "[96 rows x 2 columns]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "# Read the JSON data as a dictionary\n",
    "with open('results.json', 'r') as json_file:\n",
    "    json_data = json.load(json_file)\n",
    "\n",
    "# Convert the dictionary to a list of dictionaries\n",
    "data_list = []\n",
    "for class_name, items in json_data.items():\n",
    "    for item in items:\n",
    "        data_list.append({'Test_group': class_name, 'Test_name': item})\n",
    "# Convert the list of dictionaries to a DataFrame\n",
    "df = pd.DataFrame(data_list)\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Test_group</th>\n",
       "      <th>Test_name</th>\n",
       "      <th>Aborted_tests</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TestReadWriteDiskValidator</td>\n",
       "      <td>testReadWriteDiskValidator</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TestReadWriteDiskValidator</td>\n",
       "      <td>testReadWriteDiskValidator</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TestReadWriteDiskValidator</td>\n",
       "      <td>testReadWriteDiskValidator</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TestReadWriteDiskValidator</td>\n",
       "      <td>testReadWriteDiskValidator</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TestReadWriteDiskValidator</td>\n",
       "      <td>testCheckFailures</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>605</th>\n",
       "      <td>sql/hive</td>\n",
       "      <td>- Read/Write Hive ORC serde table *** FAILED ***</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>606</th>\n",
       "      <td>sql/hive</td>\n",
       "      <td>- SPARK-19459/SPARK-18220: read char/varchar c...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>607</th>\n",
       "      <td>sql/hive</td>\n",
       "      <td>- SPARK-8020: set sql conf in spark conf *** F...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>608</th>\n",
       "      <td>sql/hive</td>\n",
       "      <td>- SPARK-9757 Persist Parquet relation with dec...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>609</th>\n",
       "      <td>sql/hive</td>\n",
       "      <td>- SPARK-16901: set javax.jdo.option.Connection...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>610 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Test_group  \\\n",
       "0    TestReadWriteDiskValidator   \n",
       "1    TestReadWriteDiskValidator   \n",
       "2    TestReadWriteDiskValidator   \n",
       "3    TestReadWriteDiskValidator   \n",
       "4    TestReadWriteDiskValidator   \n",
       "..                          ...   \n",
       "605                    sql/hive   \n",
       "606                    sql/hive   \n",
       "607                    sql/hive   \n",
       "608                    sql/hive   \n",
       "609                    sql/hive   \n",
       "\n",
       "                                             Test_name Aborted_tests  \n",
       "0                           testReadWriteDiskValidator           NaN  \n",
       "1                           testReadWriteDiskValidator           NaN  \n",
       "2                           testReadWriteDiskValidator           NaN  \n",
       "3                           testReadWriteDiskValidator           NaN  \n",
       "4                                    testCheckFailures           NaN  \n",
       "..                                                 ...           ...  \n",
       "605   - Read/Write Hive ORC serde table *** FAILED ***           NaN  \n",
       "606  - SPARK-19459/SPARK-18220: read char/varchar c...           NaN  \n",
       "607  - SPARK-8020: set sql conf in spark conf *** F...           NaN  \n",
       "608  - SPARK-9757 Persist Parquet relation with dec...           NaN  \n",
       "609  - SPARK-16901: set javax.jdo.option.Connection...           NaN  \n",
       "\n",
       "[610 rows x 3 columns]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read the aborted-tests.txt file if it has some input in it\n",
    "if os.path.getsize('aborted-tests.txt') > 1:\n",
    "    df2 = pd.read_csv('aborted-tests.txt', header=None, sep='.txt:', engine='python')\n",
    "    # Give column names\n",
    "    df2.columns = ['Test_group','Aborted_tests']\n",
    "    # Iliminate the path to SparkTestSuite file and keep only the module name \n",
    "    df2['Test_group'] = df2['Test_group'].str.replace(\"/target/surefire-reports/SparkTestSuite\",\"\")\n",
    "    # Merge df1 and df2\n",
    "    df = pd.concat([df, df2] , ignore_index=True)\n",
    "else:\n",
    "    # Give the dataframe the column Aborted_tests with empty values\n",
    "    df[\"Aborted_tests\"] = None\n",
    "    print(\"No aborted tests\")\n",
    "\n",
    "# Read the scala-tests.txt file if it has some input in it\n",
    "if os.path.getsize('scala-tests.txt') > 1:\n",
    "    df3 = pd.read_csv('scala-tests.txt', header=None, sep='.txt:', engine='python')\n",
    "    # Give column names\n",
    "    df3.columns = ['Test_group','Test_name']\n",
    "    # Iliminate the path to SparkTestSuite file and keep only the module name \n",
    "    df3['Test_group'] = df3['Test_group'].str.replace(\"/target/surefire-reports/SparkTestSuite\",\"\")\n",
    "    # Merge new df1 and df3\n",
    "    df = pd.merge(df, df3, how='outer')\n",
    "else:\n",
    "    # Give the dataframe the column Test_name with empty colmun\n",
    "    df[\"Test_name\"] = None\n",
    "    print(\"No scala test errors\")\n",
    "\n",
    "# Concatinate the dataframe from the java tests with the new one\n",
    "#df4 = pd.concat([df, df3], ignore_index=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Test_group</th>\n",
       "      <th>Test_name</th>\n",
       "      <th>Aborted_tests</th>\n",
       "      <th>_merge</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TestReadWriteDiskValidator</td>\n",
       "      <td>testReadWriteDiskValidator</td>\n",
       "      <td>NaN</td>\n",
       "      <td>both</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TestReadWriteDiskValidator</td>\n",
       "      <td>testReadWriteDiskValidator</td>\n",
       "      <td>NaN</td>\n",
       "      <td>both</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TestReadWriteDiskValidator</td>\n",
       "      <td>testReadWriteDiskValidator</td>\n",
       "      <td>NaN</td>\n",
       "      <td>both</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TestReadWriteDiskValidator</td>\n",
       "      <td>testReadWriteDiskValidator</td>\n",
       "      <td>NaN</td>\n",
       "      <td>both</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TestReadWriteDiskValidator</td>\n",
       "      <td>testReadWriteDiskValidator</td>\n",
       "      <td>NaN</td>\n",
       "      <td>both</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>865</th>\n",
       "      <td>sql/hive</td>\n",
       "      <td>- Read/Write Hive ORC serde table *** FAILED ***</td>\n",
       "      <td>NaN</td>\n",
       "      <td>both</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>866</th>\n",
       "      <td>sql/hive</td>\n",
       "      <td>- SPARK-19459/SPARK-18220: read char/varchar c...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>both</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>867</th>\n",
       "      <td>sql/hive</td>\n",
       "      <td>- SPARK-8020: set sql conf in spark conf *** F...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>both</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>868</th>\n",
       "      <td>sql/hive</td>\n",
       "      <td>- SPARK-9757 Persist Parquet relation with dec...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>both</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>869</th>\n",
       "      <td>sql/hive</td>\n",
       "      <td>- SPARK-16901: set javax.jdo.option.Connection...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>both</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>870 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Test_group  \\\n",
       "0    TestReadWriteDiskValidator   \n",
       "1    TestReadWriteDiskValidator   \n",
       "2    TestReadWriteDiskValidator   \n",
       "3    TestReadWriteDiskValidator   \n",
       "4    TestReadWriteDiskValidator   \n",
       "..                          ...   \n",
       "865                    sql/hive   \n",
       "866                    sql/hive   \n",
       "867                    sql/hive   \n",
       "868                    sql/hive   \n",
       "869                    sql/hive   \n",
       "\n",
       "                                             Test_name Aborted_tests _merge  \n",
       "0                           testReadWriteDiskValidator           NaN   both  \n",
       "1                           testReadWriteDiskValidator           NaN   both  \n",
       "2                           testReadWriteDiskValidator           NaN   both  \n",
       "3                           testReadWriteDiskValidator           NaN   both  \n",
       "4                           testReadWriteDiskValidator           NaN   both  \n",
       "..                                                 ...           ...    ...  \n",
       "865   - Read/Write Hive ORC serde table *** FAILED ***           NaN   both  \n",
       "866  - SPARK-19459/SPARK-18220: read char/varchar c...           NaN   both  \n",
       "867  - SPARK-8020: set sql conf in spark conf *** F...           NaN   both  \n",
       "868  - SPARK-9757 Persist Parquet relation with dec...           NaN   both  \n",
       "869  - SPARK-16901: set javax.jdo.option.Connection...           NaN   both  \n",
       "\n",
       "[870 rows x 4 columns]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "comparison_run = \"0\"\n",
    "# Read the output from the transformer function\n",
    "# Read the JSON data as a dictionary\n",
    "\n",
    "\n",
    "# Load the external dataframe which we want to compare\n",
    "# If We do not give a comparison run, we compare it with the same dataset which will not give any difference\n",
    "if comparison_run == \"0\":\n",
    "    df_external = df\n",
    "# Otherwise we compare it with the dataset we give for comparison\n",
    "else:\n",
    "    df_external = pd.read_json(f'{comparison_run}')\n",
    "\n",
    "\n",
    "# Drop the columns which can hinder the comparison\n",
    "#df4 = df1.drop(columns=['Succeeded', 'Failed', 'Skipped', 'Pending'])\n",
    "#df_external = df_external.drop(columns=['Succeeded', 'Failed', 'Skipped', 'Pending'])\n",
    "\n",
    "# Merge ancient and new tables\n",
    "df = df.merge(df_external, how='left', indicator=True)\n",
    "df\n",
    "# Checks if there is an aborted test or a test error in the new table\n",
    "\n",
    "#df_comparison = df[((df['_merge'] == 'left_only') & pd.notna(df['Aborted_tests'])) | ((df['_merge'] == 'left_only') & pd.notna(df['Test_name']))]\n",
    "\n",
    "#print(\"Comparison succeeded\")\n",
    "\n",
    "# Produce csv file for comparison and entire dataframe taht we will need later on for the decision making. These are the required outputs of the function.\n",
    "#return df_comparison.to_csv('comparison.csv', header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_comparison = df[((df['_merge'] == 'left_only') & pd.notna(df['Aborted_tests'])) | ((df['_merge'] == 'left_only') & pd.notna(df['Test_name']))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Test_group</th>\n",
       "      <th>Test_name</th>\n",
       "      <th>Aborted_tests</th>\n",
       "      <th>_merge</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Test_group, Test_name, Aborted_tests, _merge]\n",
       "Index: []"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Test_group</th>\n",
       "      <th>Test_name</th>\n",
       "      <th>Aborted_tests</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TestReadWriteDiskValidator</td>\n",
       "      <td>testReadWriteDiskValidator</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TestReadWriteDiskValidator</td>\n",
       "      <td>testReadWriteDiskValidator</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TestReadWriteDiskValidator</td>\n",
       "      <td>testReadWriteDiskValidator</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TestReadWriteDiskValidator</td>\n",
       "      <td>testReadWriteDiskValidator</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TestReadWriteDiskValidator</td>\n",
       "      <td>testCheckFailures</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>605</th>\n",
       "      <td>sql/hive</td>\n",
       "      <td>- Read/Write Hive ORC serde table *** FAILED ***</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>606</th>\n",
       "      <td>sql/hive</td>\n",
       "      <td>- SPARK-19459/SPARK-18220: read char/varchar c...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>607</th>\n",
       "      <td>sql/hive</td>\n",
       "      <td>- SPARK-8020: set sql conf in spark conf *** F...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>608</th>\n",
       "      <td>sql/hive</td>\n",
       "      <td>- SPARK-9757 Persist Parquet relation with dec...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>609</th>\n",
       "      <td>sql/hive</td>\n",
       "      <td>- SPARK-16901: set javax.jdo.option.Connection...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>610 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Test_group  \\\n",
       "0    TestReadWriteDiskValidator   \n",
       "1    TestReadWriteDiskValidator   \n",
       "2    TestReadWriteDiskValidator   \n",
       "3    TestReadWriteDiskValidator   \n",
       "4    TestReadWriteDiskValidator   \n",
       "..                          ...   \n",
       "605                    sql/hive   \n",
       "606                    sql/hive   \n",
       "607                    sql/hive   \n",
       "608                    sql/hive   \n",
       "609                    sql/hive   \n",
       "\n",
       "                                             Test_name Aborted_tests  \n",
       "0                           testReadWriteDiskValidator           NaN  \n",
       "1                           testReadWriteDiskValidator           NaN  \n",
       "2                           testReadWriteDiskValidator           NaN  \n",
       "3                           testReadWriteDiskValidator           NaN  \n",
       "4                                    testCheckFailures           NaN  \n",
       "..                                                 ...           ...  \n",
       "605   - Read/Write Hive ORC serde table *** FAILED ***           NaN  \n",
       "606  - SPARK-19459/SPARK-18220: read char/varchar c...           NaN  \n",
       "607  - SPARK-8020: set sql conf in spark conf *** F...           NaN  \n",
       "608  - SPARK-9757 Persist Parquet relation with dec...           NaN  \n",
       "609  - SPARK-16901: set javax.jdo.option.Connection...           NaN  \n",
       "\n",
       "[610 rows x 3 columns]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('results.json', 'r') as json_file:\n",
    "    json_data = json.load(json_file)\n",
    "\n",
    "# Convert the dictionary to a list of dictionaries\n",
    "data_list = []\n",
    "for class_name, items in json_data.items():\n",
    "    for item in items:\n",
    "        data_list.append({'Test_group': class_name, 'Test_name': item})\n",
    "# Convert the list of dictionaries to a DataFrame\n",
    "df = pd.DataFrame(data_list)\n",
    "\n",
    "# Read the aborted-tests.txt file if it has some input in it\n",
    "if os.path.getsize('aborted-tests.txt') > 1:\n",
    "    df2 = pd.read_csv('aborted-tests.txt', header=None, sep='.txt:', engine='python')\n",
    "    # Give column names\n",
    "    df2.columns = ['Test_group','Aborted_tests']\n",
    "    # Iliminate the path to SparkTestSuite file and keep only the module name \n",
    "    df2['Test_group'] = df2['Test_group'].str.replace(\"/target/surefire-reports/SparkTestSuite\",\"\")\n",
    "    # Concatinate df1 and df2\n",
    "    df = pd.concat([df, df2] , ignore_index=True)\n",
    "else:\n",
    "    # Give the dataframe the column Aborted_tests with empty values\n",
    "    df2[\"Aborted_tests\"] = None\n",
    "    print(\"No aborted tests\")\n",
    "\n",
    "# Read the scala-tests.txt file if it has some input in it\n",
    "if os.path.getsize('scala-tests.txt') > 1:\n",
    "    df3 = pd.read_csv('scala-tests.txt', header=None, sep='.txt:', engine='python')\n",
    "    # Give column names\n",
    "    df3.columns = ['Test_group','Test_name']\n",
    "    # Iliminate the path to SparkTestSuite file and keep only the module name \n",
    "    df3['Test_group'] = df3['Test_group'].str.replace(\"/target/surefire-reports/SparkTestSuite\",\"\")\n",
    "    # Merge new df1 and df3\n",
    "    df = pd.merge(df, df3, how='outer')\n",
    "else:\n",
    "    # Give the dataframe the column Test_name with empty colmun\n",
    "    df[\"Test_name\"] = None\n",
    "    print(\"No scala test errors\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sys\n",
    "import os\n",
    "def java_test_transfomer(build_number):\n",
    "    \"\"\"\n",
    "    This function constructs the table with th test data taken from the extraction. It then returns the table in json format with all extracted information.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        \n",
    "\n",
    "        # Read the java-test-failures.txt file if it has some input in it\n",
    "        if os.path.getsize('java-test-failures.txt') > 1:\n",
    "            # Read output-tests.csv file as dataframe\n",
    "            df2 = pd.read_csv('java-test-failures.txt', header = None, delimiter='/t', engine='python')\n",
    "            # Name the single column Tests\n",
    "            df2.columns = ['Tests']\n",
    "            # Split the columns into two coulumns named Failed_test and Test_group\n",
    "            df2[['Failed_test', 'Test_group']] = df2['Tests'].str.split('(', n=1, expand= True)\n",
    "            # Remove the column Tests\n",
    "            df2.drop(columns=['Tests'], inplace= True)\n",
    "            # Remove all characters before .Test in coulumn Test_group\n",
    "            df2['Test_group'] = df2['Test_group'].str.extract(r'(\\.Test.*)', expand=False)\n",
    "            # Only extract chracter between the . and °\n",
    "            df2['Test_group'] = df2['Test_group'].str.extract(r'\\.(.*?)\\)', expand=False).str.strip()\n",
    "            # Reorder columns\n",
    "            df2= df2[['Test_group', 'Failed_test']]\n",
    "            #df = pd.merge(df, df2, on = 'Test_group', how='outer')\n",
    "        else:\n",
    "            # Create the column Failed_test with empty values\n",
    "            df2['Failed_test'] = None\n",
    "            print(\"No java failed tests\")\n",
    "\n",
    "        \n",
    "        # Write the rsultin json format where each element of Failed_test belongs to the class Test_group\n",
    "        # create a dictionnary\n",
    "        json_dict = {}\n",
    "        # Go through each row\n",
    "        for _, row in df2.iterrows():\n",
    "            # define the columns\n",
    "            class_name = row['Test_group']\n",
    "            item_name = row['Failed_test']\n",
    "            # if the classname does not yet exist in the dictionnary append it as a list\n",
    "            if class_name not in json_dict:\n",
    "                json_dict[class_name] = []\n",
    "            # Append each item corresponding to the test belonging to the test class in the list of the test class\n",
    "            json_dict[class_name].append(item_name)\n",
    "\n",
    "        # Convert the dictionary to a JSON file\n",
    "        json_data = pd.io.json.dumps(json_dict, indent=4)\n",
    "\n",
    "        # Write the JSON data to a file\n",
    "        with open('results.json', 'w') as json_file:\n",
    "            json_file.write(json_data)\n",
    "        \n",
    "        print(\"Java data transformation succeeded\")\n",
    "\n",
    "    except:\n",
    "        print(\"Java data transformation failed\")\n",
    "        sys.exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "build_number= 8\n",
    "\n",
    "# Read the output from the scala transformer\n",
    "#df = pd.read_json('results.json')\n",
    "\n",
    "# Read the JSON data as a dictionary\n",
    "with open('results.json', 'r') as json_file:\n",
    "    json_data = json.load(json_file)\n",
    "\n",
    "# Convert the dictionary to a list of dictionaries\n",
    "data_list = []\n",
    "for class_name, items in json_data.items():\n",
    "    for item in items:\n",
    "        data_list.append({'Test_group': class_name, 'Failed_test': item})\n",
    "# Convert the list of dictionaries to a DataFrame\n",
    "df = pd.DataFrame(data_list)\n",
    "\n",
    "# Read the aborted-tests.txt file if it has some input in it\n",
    "if os.path.getsize('aborted-tests.txt') > 1:\n",
    "    df2 = pd.read_csv('aborted-tests.txt', header=None, sep='.txt:', engine='python')\n",
    "    # Give column names\n",
    "    df2.columns = ['Test_group','Aborted_tests']\n",
    "    # Iliminate the path to SparkTestSuite file and keep only the module name \n",
    "    df2['Test_group'] = df2['Test_group'].str.replace(\"/target/surefire-reports/SparkTestSuite\",\"\")\n",
    "    # Merge df1 and df2\n",
    "    df = pd.concat([df, df2] , ignore_index=True)\n",
    "else:\n",
    "    # Give the dataframe the column Aborted_tests with empty values\n",
    "    df[\"Aborted_tests\"] = None\n",
    "    print(\"No aborted tests\")\n",
    "\n",
    "# Read the scala-tests.txt file if it has some input in it\n",
    "if os.path.getsize('scala-tests.txt') > 1:\n",
    "    df3 = pd.read_csv('scala-tests.txt', header=None, sep='.txt:', engine='python')\n",
    "    # Give column names\n",
    "    df3.columns = ['Test_group','Failed_test']\n",
    "    # Iliminate the path to SparkTestSuite file and keep only the module name \n",
    "    df3['Test_group'] = df3['Test_group'].str.replace(\"/target/surefire-reports/SparkTestSuite\",\"\")\n",
    "    # Merge new df1 and df3\n",
    "    df = pd.merge(df, df3, how='outer')\n",
    "else:\n",
    "    # Give the dataframe the column Failed_test with empty colmun\n",
    "    df[\"Failed_test\"] = None\n",
    "    print(\"No scala test errors\")\n",
    "df\n",
    "\n",
    "json_dict = {}\n",
    "    # Go through each row\n",
    "for _, row in df.iterrows():\n",
    "    # define the columns\n",
    "    class_name = row['Test_group']\n",
    "    item_name = row['Failed_test']\n",
    "    property_value = row['Aborted_tests']\n",
    "    # if the classname does not yet exist in the dictionnary append it as a list\n",
    "    if class_name not in json_dict:\n",
    "        json_dict[class_name] = []\n",
    "    # Append each item corresponding to the test belonging to the test class in the list of the test class\n",
    "    json_dict[class_name].append({'Failed_test': item_name, 'Aborted_tests': property_value})\n",
    "\n",
    "# Convert the dictionary to a JSON file\n",
    "json_data = pd.io.json.dumps(json_dict, indent=4)\n",
    "\n",
    "# Write the JSON data to a file\n",
    "with open(f'results-{build_number}.json', 'w') as json_file:\n",
    "    json_file.write(json_data)\n",
    "\n",
    "# Concatinate the dataframe from the java tests with the new one\n",
    "#df4 = pd.concat([df, df1], ignore_index=True)\n",
    "\n",
    "# Write the rsultin json format where each element of Failed_test belongs to the class Test_group\n",
    "# create a dictionnary\n",
    "\n",
    "\n",
    "# The output of this function is the total results file combined with one from the java test function transformer\n",
    "#return df4.to_json(f'results-{build_number}.json')\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Java data transformation succeeded\n",
      "Scala data transfromation succeeded\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Test_group</th>\n",
       "      <th>Failed_test</th>\n",
       "      <th>Aborted_tests</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TestReadWriteDiskValidator</td>\n",
       "      <td>Failed_test</td>\n",
       "      <td>Aborted_tests</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TestReadWriteDiskValidator</td>\n",
       "      <td>Failed_test</td>\n",
       "      <td>Aborted_tests</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TestReadWriteDiskValidator</td>\n",
       "      <td>Failed_test</td>\n",
       "      <td>Aborted_tests</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TestReadWriteDiskValidator</td>\n",
       "      <td>Failed_test</td>\n",
       "      <td>Aborted_tests</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TestReadWriteDiskValidator</td>\n",
       "      <td>Failed_test</td>\n",
       "      <td>Aborted_tests</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>602</th>\n",
       "      <td>sql/hive</td>\n",
       "      <td>Failed_test</td>\n",
       "      <td>Aborted_tests</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>603</th>\n",
       "      <td>sql/hive</td>\n",
       "      <td>Failed_test</td>\n",
       "      <td>Aborted_tests</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>604</th>\n",
       "      <td>sql/hive</td>\n",
       "      <td>Failed_test</td>\n",
       "      <td>Aborted_tests</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>605</th>\n",
       "      <td>sql/hive</td>\n",
       "      <td>Failed_test</td>\n",
       "      <td>Aborted_tests</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>606</th>\n",
       "      <td>sql/hive</td>\n",
       "      <td>Failed_test</td>\n",
       "      <td>Aborted_tests</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>607 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Test_group  Failed_test  Aborted_tests\n",
       "0    TestReadWriteDiskValidator  Failed_test  Aborted_tests\n",
       "1    TestReadWriteDiskValidator  Failed_test  Aborted_tests\n",
       "2    TestReadWriteDiskValidator  Failed_test  Aborted_tests\n",
       "3    TestReadWriteDiskValidator  Failed_test  Aborted_tests\n",
       "4    TestReadWriteDiskValidator  Failed_test  Aborted_tests\n",
       "..                          ...          ...            ...\n",
       "602                    sql/hive  Failed_test  Aborted_tests\n",
       "603                    sql/hive  Failed_test  Aborted_tests\n",
       "604                    sql/hive  Failed_test  Aborted_tests\n",
       "605                    sql/hive  Failed_test  Aborted_tests\n",
       "606                    sql/hive  Failed_test  Aborted_tests\n",
       "\n",
       "[607 rows x 3 columns]"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "java_test_transfomer(8)\n",
    "scala_transfomer_fy(8)\n",
    "build_number= 5\n",
    "comparison_run= 'results-5.json'\n",
    "# Read the output from the transformer function\n",
    "# Read the JSON data as a dictionary\n",
    "with open(f'results-{build_number}.json', 'r') as json_file:\n",
    "    json_data = json.load(json_file)\n",
    "\n",
    "# Convert the dictionary to a list of dictionaries\n",
    "data_list = []\n",
    "for class_name, items in json_data.items():\n",
    "    for item, item2 in items:\n",
    "        data_list.append({'Test_group': class_name, 'Failed_test': item, 'Aborted_tests': item2})\n",
    "# Convert the list of dictionaries to a DataFrame\n",
    "df = pd.DataFrame(data_list)\n",
    "\n",
    "# Load the external dataframe which we want to compare\n",
    "# If We do not give a comparison run, we compare it with the same dataset which will not give any difference\n",
    "if comparison_run == \"0\":\n",
    "    df_external = df\n",
    "\n",
    "df\n",
    "#print(df_external)\n",
    "#print(df)\n",
    "# Merge ancient and new tables\n",
    "#df = df.merge(df_external, how='left', indicator=True)\n",
    "# Checks if there is an aborted test or a test error in the new table\n",
    "#print(df[(df['_merge'] == 'left_only')])\n",
    "#df_comparison = df[((df['_merge'] == 'left_only') & pd.notna(df['Aborted_tests'])) | ((df['_merge'] == 'left_only') & pd.notna(df['Failed_test']))]\n",
    "#print(\"Comparison succeeded\")\n",
    "#print(df_comparison)\n",
    "# Produce csv file for comparison and entire dataframe taht we will need later on for the decision making. These are the required outputs of the function.\n",
    "#return df_comparison.to_csv('comparison.csv', header=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Java data transformation succeeded\n",
      "Scala data transfromation succeeded\n",
      "Comparison succeeded\n"
     ]
    }
   ],
   "source": [
    "java_test_transfomer(8)\n",
    "scala_transfomer_fy(8)\n",
    "comparison_producer(8, 'results-5.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('output-tests.csv')\n",
    "# Give column names\n",
    "df.columns = ['Tests_run', 'Failures', 'Errors', 'Skipped', 'Test_group']\n",
    "# Erase all unnecessary characters in each column and transform them into type int64\n",
    "df['Tests_run'] = df['Tests_run'].str.split(':', n=1).str[1].astype('int64')\n",
    "df['Failures'] = df['Failures'].str.split(':', n=1).str[1].astype('int64')\n",
    "df['Errors'] = df['Errors'].str.split(':', n=1).str[1].astype('int64')\n",
    "df['Skipped'] = df['Skipped'].str.split(':', n=1).str[1].astype('int64')\n",
    "df['Test_group'] = df['Test_group'].str.split('-', n=1).str[1].str.replace('in','')\n",
    "# delete lines where no testname is given which corresponds to the total columns\n",
    "df = df.dropna(subset=['Test_group'])\n",
    "# Add empty columns below to be compatible with schema\n",
    "df['Pending'] = [None] * len(df)\n",
    "df['Aborted_tests'] = [None] * len(df)\n",
    "df['Failed']= df['Failures'] + df['Errors']\n",
    "# Succeeded is the nuber of test runs minus failed and skipped test\n",
    "df['Succeeded'] = df['Tests_run'] - df['Failed'] - df['Skipped']\n",
    "# Select and reorder the columns\n",
    "df= df[['Test_group', 'Succeeded', 'Failed', 'Skipped', 'Pending', 'Aborted_tests']]\n",
    "df.to_csv('modified-output-tests.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Test_group</th>\n",
       "      <th>Succeeded</th>\n",
       "      <th>Failed</th>\n",
       "      <th>Skipped</th>\n",
       "      <th>Pending</th>\n",
       "      <th>Aborted_tests</th>\n",
       "      <th>Failed_test</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>org.apache.spark.util.kvstore.InMemoryStoreS...</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>org.apache.spark.util.kvstore.ArrayWrappersS...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>org.apache.spark.util.kvstore.LevelDBIterato...</td>\n",
       "      <td>38.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>org.apache.spark.util.kvstore.InMemoryIterat...</td>\n",
       "      <td>38.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>org.apache.spark.util.kvstore.LevelDBTypeInf...</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249</th>\n",
       "      <td>org.apache.hadoop.mapred.TestMRTimelineEventHa...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>testMRNewTimelineServiceEventHandling</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>250</th>\n",
       "      <td>org.apache.hadoop.mapred.TestMRTimelineEventHa...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>testMRNewTimelineServiceEventHandling</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>251</th>\n",
       "      <td>org.apache.hadoop.mapred.TestMRTimelineEventHa...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>testMRNewTimelineServiceEventHandling</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>252</th>\n",
       "      <td>org.apache.hadoop.mapred.TestMRTimelineEventHa...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>testMRNewTimelineServiceEventHandling</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>253</th>\n",
       "      <td>nodeFile null</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>testSimulatorRunning[Testing with: SYNTH, org....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>254 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Test_group  Succeeded  Failed  \\\n",
       "0      org.apache.spark.util.kvstore.InMemoryStoreS...        6.0     0.0   \n",
       "1      org.apache.spark.util.kvstore.ArrayWrappersS...        1.0     0.0   \n",
       "2      org.apache.spark.util.kvstore.LevelDBIterato...       38.0     0.0   \n",
       "3      org.apache.spark.util.kvstore.InMemoryIterat...       38.0     0.0   \n",
       "4      org.apache.spark.util.kvstore.LevelDBTypeInf...       10.0     0.0   \n",
       "..                                                 ...        ...     ...   \n",
       "249  org.apache.hadoop.mapred.TestMRTimelineEventHa...        NaN     NaN   \n",
       "250  org.apache.hadoop.mapred.TestMRTimelineEventHa...        NaN     NaN   \n",
       "251  org.apache.hadoop.mapred.TestMRTimelineEventHa...        NaN     NaN   \n",
       "252  org.apache.hadoop.mapred.TestMRTimelineEventHa...        NaN     NaN   \n",
       "253                                      nodeFile null        NaN     NaN   \n",
       "\n",
       "     Skipped Pending Aborted_tests  \\\n",
       "0        0.0    None          None   \n",
       "1        0.0    None          None   \n",
       "2        0.0    None          None   \n",
       "3        0.0    None          None   \n",
       "4        0.0    None          None   \n",
       "..       ...     ...           ...   \n",
       "249      NaN     NaN           NaN   \n",
       "250      NaN     NaN           NaN   \n",
       "251      NaN     NaN           NaN   \n",
       "252      NaN     NaN           NaN   \n",
       "253      NaN     NaN           NaN   \n",
       "\n",
       "                                           Failed_test  \n",
       "0                                                  NaN  \n",
       "1                                                  NaN  \n",
       "2                                                  NaN  \n",
       "3                                                  NaN  \n",
       "4                                                  NaN  \n",
       "..                                                 ...  \n",
       "249              testMRNewTimelineServiceEventHandling  \n",
       "250              testMRNewTimelineServiceEventHandling  \n",
       "251              testMRNewTimelineServiceEventHandling  \n",
       "252              testMRNewTimelineServiceEventHandling  \n",
       "253  testSimulatorRunning[Testing with: SYNTH, org....  \n",
       "\n",
       "[254 rows x 7 columns]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read output-tests.csv file as dataframe\n",
    "df2 = pd.read_csv('java-test-failures.txt', header = None, delimiter='/t', engine='python')\n",
    "# Name the single column Tests\n",
    "df2.columns = ['Tests']\n",
    "# Split the columns into two coulumns named Failed_test and Test_group\n",
    "df2[['Failed_test', 'Test_group']] = df2['Tests'].str.split('(', n=1, expand= True)\n",
    "# Remove the column Tests\n",
    "df2.drop(columns=['Tests'], inplace= True)\n",
    "# Split the columns Test_group into two after the )\n",
    "df2[['Test_group', 'unnecsessary']] = df2['Test_group'].str.split(')', n=1, expand= True)\n",
    "# Remove evrything after the parnetheses\n",
    "df2.drop(columns=['unnecsessary'], inplace= True)\n",
    "# Remove all characters before .Test in coulumn Test_group\n",
    "#df2['Test_group'] = df2['Test_group'].str.extract('.*\\((.*\\d{4})\\).*', expand=False)\n",
    "# Only extract chracter between the . and °\n",
    "#df2['Test_group'] = df2['Test_group'].str.extract('\\.(.*?)\\)', expand=False).str.strip()\n",
    "\n",
    "#df2['Test_group'] = df2['Test_group'].str.split\n",
    "# Reorder columns\n",
    "df2= df2[['Test_group', 'Failed_test']]\n",
    "df = pd.merge(df, df2, on = 'Test_group', how='outer')\n",
    "#df2.to_csv(\"modified-java-test-failures.txt\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Test_group</th>\n",
       "      <th>Succeeded</th>\n",
       "      <th>Failed</th>\n",
       "      <th>Skipped</th>\n",
       "      <th>Pending</th>\n",
       "      <th>Aborted_tests</th>\n",
       "      <th>Failed_test</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>org.apache.spark.util.kvstore.InMemoryStoreS...</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>org.apache.spark.util.kvstore.ArrayWrappersS...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>org.apache.spark.util.kvstore.LevelDBIterato...</td>\n",
       "      <td>38.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>org.apache.spark.util.kvstore.InMemoryIterat...</td>\n",
       "      <td>38.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>org.apache.spark.util.kvstore.LevelDBTypeInf...</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249</th>\n",
       "      <td>org.apache.hadoop.mapred.TestMRTimelineEventHa...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>testMRNewTimelineServiceEventHandling</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>250</th>\n",
       "      <td>org.apache.hadoop.mapred.TestMRTimelineEventHa...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>testMRNewTimelineServiceEventHandling</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>251</th>\n",
       "      <td>org.apache.hadoop.mapred.TestMRTimelineEventHa...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>testMRNewTimelineServiceEventHandling</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>252</th>\n",
       "      <td>org.apache.hadoop.mapred.TestMRTimelineEventHa...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>testMRNewTimelineServiceEventHandling</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>253</th>\n",
       "      <td>nodeFile null</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>testSimulatorRunning[Testing with: SYNTH, org....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>254 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Test_group  Succeeded  Failed  \\\n",
       "0      org.apache.spark.util.kvstore.InMemoryStoreS...        6.0     0.0   \n",
       "1      org.apache.spark.util.kvstore.ArrayWrappersS...        1.0     0.0   \n",
       "2      org.apache.spark.util.kvstore.LevelDBIterato...       38.0     0.0   \n",
       "3      org.apache.spark.util.kvstore.InMemoryIterat...       38.0     0.0   \n",
       "4      org.apache.spark.util.kvstore.LevelDBTypeInf...       10.0     0.0   \n",
       "..                                                 ...        ...     ...   \n",
       "249  org.apache.hadoop.mapred.TestMRTimelineEventHa...        NaN     NaN   \n",
       "250  org.apache.hadoop.mapred.TestMRTimelineEventHa...        NaN     NaN   \n",
       "251  org.apache.hadoop.mapred.TestMRTimelineEventHa...        NaN     NaN   \n",
       "252  org.apache.hadoop.mapred.TestMRTimelineEventHa...        NaN     NaN   \n",
       "253                                      nodeFile null        NaN     NaN   \n",
       "\n",
       "     Skipped Pending Aborted_tests  \\\n",
       "0        0.0    None          None   \n",
       "1        0.0    None          None   \n",
       "2        0.0    None          None   \n",
       "3        0.0    None          None   \n",
       "4        0.0    None          None   \n",
       "..       ...     ...           ...   \n",
       "249      NaN     NaN           NaN   \n",
       "250      NaN     NaN           NaN   \n",
       "251      NaN     NaN           NaN   \n",
       "252      NaN     NaN           NaN   \n",
       "253      NaN     NaN           NaN   \n",
       "\n",
       "                                           Failed_test  \n",
       "0                                                  NaN  \n",
       "1                                                  NaN  \n",
       "2                                                  NaN  \n",
       "3                                                  NaN  \n",
       "4                                                  NaN  \n",
       "..                                                 ...  \n",
       "249              testMRNewTimelineServiceEventHandling  \n",
       "250              testMRNewTimelineServiceEventHandling  \n",
       "251              testMRNewTimelineServiceEventHandling  \n",
       "252              testMRNewTimelineServiceEventHandling  \n",
       "253  testSimulatorRunning[Testing with: SYNTH, org....  \n",
       "\n",
       "[254 rows x 7 columns]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read output-tests.csv file as dataframe\n",
    "df = pd.read_csv('output-tests.csv')\n",
    "# Give column names\n",
    "df.columns = ['Tests_run', 'Failures', 'Errors', 'Skipped', 'Test_group']\n",
    "# Erase all unnecessary characters in each column and transform them into type int64\n",
    "df['Tests_run'] = df['Tests_run'].str.split(':', n=1).str[1].astype('int64')\n",
    "df['Failures'] = df['Failures'].str.split(':', n=1).str[1].astype('int64')\n",
    "df['Errors'] = df['Errors'].str.split(':', n=1).str[1].astype('int64')\n",
    "df['Skipped'] = df['Skipped'].str.split(':', n=1).str[1].astype('int64')\n",
    "df['Test_group'] = df['Test_group'].str.split('-', n=1).str[1].str.replace('in','')\n",
    "# delete lines where no testname is given which corresponds to the total columns\n",
    "df = df.dropna(subset=['Test_group'])\n",
    "# Add empty columns below to be compatible with schema\n",
    "df['Pending'] = [None] * len(df)\n",
    "df['Aborted_tests'] = [None] * len(df)\n",
    "df['Failed']= df['Failures'] + df['Errors']\n",
    "# Succeeded is the nuber of test runs minus failed and skipped test\n",
    "df['Succeeded'] = df['Tests_run'] - df['Failed'] - df['Skipped']\n",
    "# Select and reorder the columns\n",
    "df= df[['Test_group', 'Succeeded', 'Failed', 'Skipped', 'Pending', 'Aborted_tests']]\n",
    "\n",
    "# Read the java-test-failures.txt file if it has some input in it\n",
    "if os.path.getsize('java-test-failures.txt') > 1:\n",
    "    # Read output-tests.csv file as dataframe\n",
    "    df2 = pd.read_csv('java-test-failures.txt', header = None, delimiter='/t', engine='python')\n",
    "    # Name the single column Tests\n",
    "    df2.columns = ['Tests']\n",
    "    # Split the columns into two coulumns named Failed_test and Test_group\n",
    "    df2[['Failed_test', 'Test_group']] = df2['Tests'].str.split('(', n=1, expand= True)\n",
    "    # Remove the column Tests\n",
    "    df2.drop(columns=['Tests'], inplace= True)\n",
    "    # Split the columns Test_group into two after the )\n",
    "    df2[['Test_group', 'unnecsessary']] = df2['Test_group'].str.split(')', n=1, expand= True)\n",
    "    # Remove evrything after the parnetheses\n",
    "    df2.drop(columns=['unnecsessary'], inplace= True)\n",
    "    # Reorder columns\n",
    "    df2= df2[['Test_group', 'Failed_test']]\n",
    "    df = pd.merge(df, df2, on = 'Test_group', how='outer')\n",
    "else:\n",
    "    # Create the column Failed_test with empty values\n",
    "    df['Failed_test'] = None\n",
    "    print(\"No java failed tests\")\n",
    "\n",
    "df.to_json(\"testing.json\", orient=\"records\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Test_group</th>\n",
       "      <th>Succeeded</th>\n",
       "      <th>Failed</th>\n",
       "      <th>Skipped</th>\n",
       "      <th>Pending</th>\n",
       "      <th>Aborted_tests</th>\n",
       "      <th>Failed_test</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>org.apache.spark.util.kvstore.InMemoryStoreS...</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>org.apache.spark.util.kvstore.ArrayWrappersS...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>org.apache.spark.util.kvstore.LevelDBIterato...</td>\n",
       "      <td>38.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>org.apache.spark.util.kvstore.InMemoryIterat...</td>\n",
       "      <td>38.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>org.apache.spark.util.kvstore.LevelDBTypeInf...</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>786</th>\n",
       "      <td>sql/hive</td>\n",
       "      <td>2344.0</td>\n",
       "      <td>510.0</td>\n",
       "      <td>595.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>- SPARK-19459/SPARK-18220: read char/varchar c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>787</th>\n",
       "      <td>sql/hive</td>\n",
       "      <td>2344.0</td>\n",
       "      <td>510.0</td>\n",
       "      <td>595.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>- SPARK-8020: set sql conf in spark conf *** F...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>788</th>\n",
       "      <td>sql/hive</td>\n",
       "      <td>2344.0</td>\n",
       "      <td>510.0</td>\n",
       "      <td>595.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>- SPARK-9757 Persist Parquet relation with dec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>789</th>\n",
       "      <td>sql/hive</td>\n",
       "      <td>2344.0</td>\n",
       "      <td>510.0</td>\n",
       "      <td>595.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>- SPARK-16901: set javax.jdo.option.Connection...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>790</th>\n",
       "      <td>sql/hive-thriftserver</td>\n",
       "      <td>40.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>791 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Test_group  Succeeded  Failed  \\\n",
       "0      org.apache.spark.util.kvstore.InMemoryStoreS...        6.0     0.0   \n",
       "1      org.apache.spark.util.kvstore.ArrayWrappersS...        1.0     0.0   \n",
       "2      org.apache.spark.util.kvstore.LevelDBIterato...       38.0     0.0   \n",
       "3      org.apache.spark.util.kvstore.InMemoryIterat...       38.0     0.0   \n",
       "4      org.apache.spark.util.kvstore.LevelDBTypeInf...       10.0     0.0   \n",
       "..                                                 ...        ...     ...   \n",
       "786                                           sql/hive     2344.0   510.0   \n",
       "787                                           sql/hive     2344.0   510.0   \n",
       "788                                           sql/hive     2344.0   510.0   \n",
       "789                                           sql/hive     2344.0   510.0   \n",
       "790                              sql/hive-thriftserver       40.0     0.0   \n",
       "\n",
       "     Skipped  Pending Aborted_tests  \\\n",
       "0        0.0      NaN           NaN   \n",
       "1        0.0      NaN           NaN   \n",
       "2        0.0      NaN           NaN   \n",
       "3        0.0      NaN           NaN   \n",
       "4        0.0      NaN           NaN   \n",
       "..       ...      ...           ...   \n",
       "786    595.0      0.0           NaN   \n",
       "787    595.0      0.0           NaN   \n",
       "788    595.0      0.0           NaN   \n",
       "789    595.0      0.0           NaN   \n",
       "790      2.0      0.0           NaN   \n",
       "\n",
       "                                           Failed_test  \n",
       "0                                                 None  \n",
       "1                                                 None  \n",
       "2                                                 None  \n",
       "3                                                 None  \n",
       "4                                                 None  \n",
       "..                                                 ...  \n",
       "786  - SPARK-19459/SPARK-18220: read char/varchar c...  \n",
       "787  - SPARK-8020: set sql conf in spark conf *** F...  \n",
       "788  - SPARK-9757 Persist Parquet relation with dec...  \n",
       "789  - SPARK-16901: set javax.jdo.option.Connection...  \n",
       "790                                                NaN  \n",
       "\n",
       "[791 rows x 7 columns]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_json(\"testing.json\", orient= \"records\")\n",
    "\n",
    "\n",
    "# Read the scala-end-results.txt file if it has some input in it\n",
    "if os.path.getsize('scala-end-results.txt') > 1:\n",
    "    # Read the scala-end-results.txt file\n",
    "    df1 = pd.read_csv('scala-end-results.txt', header=None, sep='.txt:', engine='python')\n",
    "    # Give column names\n",
    "    df1.columns = ['Test_group','Results']\n",
    "    # Iliminate the path to SparkTestSuite file and keep only the module name \n",
    "    df1['Test_group'] = df1['Test_group'].str.replace(\"/target/surefire-reports/SparkTestSuite\",\"\")\n",
    "    # Iliminate all strings in Results column\n",
    "    df1['Results'] = df1['Results'].str.replace(\"Tests: succeeded\",\"\").str.replace(\"failed\",\"\").str.replace(\"canceled\",\"\").str.replace(\"ignored\",\"\").str.replace(\"pending\",\"\")\n",
    "    # Split the Results column into sevral columns\n",
    "    splitted_columns = df1['Results'].str.split(',', expand=True)\n",
    "    # Give column names\n",
    "    splitted_columns.columns = ['Succeeded', 'Failed', 'Canceled', 'Ignored', 'Pending']\n",
    "    # Transform string to integers\n",
    "    splitted_columns = splitted_columns.astype(int)\n",
    "    # Concatinate new df\n",
    "    df1 = pd.concat([df1, splitted_columns], axis = 1).drop(columns = ['Results'])\n",
    "    # Join the columns Canceled and Ignored in one signle coulumn\n",
    "    df1['Skipped'] = df1['Canceled'] + df1['Ignored']\n",
    "    # Drop the columns which are not needed anymore\n",
    "    df1 = df1.drop(columns = ['Canceled', 'Ignored'])\n",
    "    # Reorder the columns\n",
    "    df1 = df1[['Test_group', 'Succeeded', 'Failed', 'Skipped', 'Pending']]\n",
    "else:\n",
    "    # Exit since no test has run\n",
    "    columns = ['Test_group', 'Succeeded', 'Failed', 'Skipped', 'Pending']\n",
    "    df1 = pd.DataFrame(columns = columns)\n",
    "    print(\"No module has run\")\n",
    "\n",
    "# Read the aborted-tests.txt file if it has some input in it\n",
    "if os.path.getsize('aborted-tests.txt') > 1:\n",
    "    df2 = pd.read_csv('aborted-tests.txt', header=None, sep='.txt:', engine='python')\n",
    "    # Give column names\n",
    "    df2.columns = ['Test_group','Aborted_tests']\n",
    "    # Iliminate the path to SparkTestSuite file and keep only the module name \n",
    "    df2['Test_group'] = df2['Test_group'].str.replace(\"/target/surefire-reports/SparkTestSuite\",\"\")\n",
    "    # Merge df1 and df2\n",
    "    df1 = pd.concat([df1, df2] , ignore_index=True)\n",
    "else:\n",
    "    # Give the dataframe the column Aborted_tests with empty values\n",
    "    df1[\"Aborted_tests\"] = None\n",
    "    print(\"No aborted tests\")\n",
    "\n",
    "# Read the scala-tests.txt file if it has some input in it\n",
    "if os.path.getsize('scala-tests.txt') > 1:\n",
    "    df3 = pd.read_csv('scala-tests.txt', header=None, sep='.txt:', engine='python')\n",
    "    # Give column names\n",
    "    df3.columns = ['Test_group','Failed_test']\n",
    "    # Iliminate the path to SparkTestSuite file and keep only the module name \n",
    "    df3['Test_group'] = df3['Test_group'].str.replace(\"/target/surefire-reports/SparkTestSuite\",\"\")\n",
    "    # Merge new df1 and df3\n",
    "    df1 = pd.merge(df1, df3, how='outer')\n",
    "else:\n",
    "    # Give the dataframe the column Failed_test with empty colmun\n",
    "    df1[\"Failed_test\"] = None\n",
    "    print(\"No scala test errors\")\n",
    "\n",
    "# Merge df and df1\n",
    "df = pd.concat([df, df1] , ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# read output-tests.csv file as dataframe\n",
    "df = pd.read_csv('output-tests.csv')\n",
    "# Give column names\n",
    "df.columns = ['Tests_run', 'Failures', 'Errors', 'Skipped', 'Test_group']\n",
    "# Erase all unnecessary characters in each column and transform them into type int64\n",
    "df['Tests_run'] = df['Tests_run'].str.split(':', n=1).str[1].astype('int64')\n",
    "df['Failures'] = df['Failures'].str.split(':', n=1).str[1].astype('int64')\n",
    "df['Errors'] = df['Errors'].str.split(':', n=1).str[1].astype('int64')\n",
    "df['Skipped'] = df['Skipped'].str.split(':', n=1).str[1].astype('int64')\n",
    "df['Test_group'] = df['Test_group'].str.split('-', n=1).str[1].str.replace('in','')\n",
    "# delete lines where no testname is given which corresponds to the total columns\n",
    "df = df.dropna(subset=['Test_group'])\n",
    "# Add empty columns below to be compatible with schema\n",
    "df['Pending'] = [None] * len(df)\n",
    "df['Aborted_tests'] = [None] * len(df)\n",
    "df['Failed']= df['Failures'] + df['Errors']\n",
    "# Succeeded is the nuber of test runs minus failed and skipped test\n",
    "df['Succeeded'] = df['Tests_run'] - df['Failed'] - df['Skipped']\n",
    "# Select and reorder the columns\n",
    "df= df[['Test_group', 'Succeeded', 'Failed', 'Skipped', 'Pending', 'Aborted_tests']]\n",
    "\n",
    "# Read the java-test-failures.txt file if it has some input in it\n",
    "if os.path.getsize('java-test-failures.txt') > 1:\n",
    "    # Read output-tests.csv file as dataframe\n",
    "    df2 = pd.read_csv('java-test-failures.txt', header = None, delimiter='/t', engine='python')\n",
    "    # Name the single column Tests\n",
    "    df2.columns = ['Tests']\n",
    "    # Split the columns into two coulumns named Failed_test and Test_group\n",
    "    df2[['Failed_test', 'Test_group']] = df2['Tests'].str.split('(', n=1, expand= True)\n",
    "    # Remove the column Tests\n",
    "    df2.drop(columns=['Tests'], inplace= True)\n",
    "    # Split the columns Test_group into two after the )\n",
    "    df2[['Test_group', 'unnecsessary']] = df2['Test_group'].str.split(')', n=1, expand= True)\n",
    "    # Remove evrything after the parnetheses\n",
    "    df2.drop(columns=['unnecsessary'], inplace= True)\n",
    "    # Reorder columns\n",
    "    df2= df2[['Test_group', 'Failed_test']]\n",
    "    df = pd.merge(df, df2, on = 'Test_group', how='outer')\n",
    "else:\n",
    "    # Create the column Failed_test with empty values\n",
    "    df['Failed_test'] = None\n",
    "    print(\"No java failed tests\")\n",
    "\n",
    "\n",
    "nested_dict = {}\n",
    "for _, row in df.iterrows():\n",
    "    test_group = row['Test_group']\n",
    "    if test_group not in nested_dict:\n",
    "        nested_dict[test_group] = {\n",
    "            'attributes': {\n",
    "                'Succeeded': row['Succeeded'],\n",
    "                'Failed': row['Failed'],\n",
    "                'Skipped': row['Skipped'],\n",
    "                'Pending': row['Pending'],\n",
    "                'Aborted_tests' : row['Aborted_tests']\n",
    "            },\n",
    "            'Failed_tests': []\n",
    "        }\n",
    "    nested_dict[test_group]['Failed_tests'].append(row['Failed_test'])\n",
    "\n",
    "# Write the nested dictionary to a JSON file\n",
    "with open('output.json', 'w') as json_file:\n",
    "    json.dump(nested_dict, json_file, indent=2)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'Series'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/stephan/Documents/github/tdp-alliage-jenkins/spark/test_comparison/test.ipynb Cell 18\u001b[0m in \u001b[0;36m<cell line: 69>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/stephan/Documents/github/tdp-alliage-jenkins/spark/test_comparison/test.ipynb#X23sZmlsZQ%3D%3D?line=66'>67</a>\u001b[0m \u001b[39m# Update existing data with new data\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/stephan/Documents/github/tdp-alliage-jenkins/spark/test_comparison/test.ipynb#X23sZmlsZQ%3D%3D?line=67'>68</a>\u001b[0m test_group \u001b[39m=\u001b[39m df1[\u001b[39m'\u001b[39m\u001b[39mTest_group\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/stephan/Documents/github/tdp-alliage-jenkins/spark/test_comparison/test.ipynb#X23sZmlsZQ%3D%3D?line=68'>69</a>\u001b[0m \u001b[39mif\u001b[39;00m test_group \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m existing_data:\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/stephan/Documents/github/tdp-alliage-jenkins/spark/test_comparison/test.ipynb#X23sZmlsZQ%3D%3D?line=69'>70</a>\u001b[0m     existing_data[test_group] \u001b[39m=\u001b[39m {\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/stephan/Documents/github/tdp-alliage-jenkins/spark/test_comparison/test.ipynb#X23sZmlsZQ%3D%3D?line=70'>71</a>\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mattributes\u001b[39m\u001b[39m'\u001b[39m: {\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/stephan/Documents/github/tdp-alliage-jenkins/spark/test_comparison/test.ipynb#X23sZmlsZQ%3D%3D?line=71'>72</a>\u001b[0m             \u001b[39m'\u001b[39m\u001b[39mSucceeded\u001b[39m\u001b[39m'\u001b[39m: df1[\u001b[39m'\u001b[39m\u001b[39mSucceeded\u001b[39m\u001b[39m'\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/stephan/Documents/github/tdp-alliage-jenkins/spark/test_comparison/test.ipynb#X23sZmlsZQ%3D%3D?line=77'>78</a>\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mFailed_tests\u001b[39m\u001b[39m'\u001b[39m: []\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/stephan/Documents/github/tdp-alliage-jenkins/spark/test_comparison/test.ipynb#X23sZmlsZQ%3D%3D?line=78'>79</a>\u001b[0m     }\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/stephan/Documents/github/tdp-alliage-jenkins/spark/test_comparison/test.ipynb#X23sZmlsZQ%3D%3D?line=79'>80</a>\u001b[0m existing_data[test_group][\u001b[39m'\u001b[39m\u001b[39mFailed_tests\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m df1[\u001b[39m'\u001b[39m\u001b[39mFailed_test\u001b[39m\u001b[39m'\u001b[39m]\n",
      "\u001b[0;31mTypeError\u001b[0m: unhashable type: 'Series'"
     ]
    }
   ],
   "source": [
    "# Read the scala-end-results.txt file if it has some input in it\n",
    "if os.path.getsize('scala-end-results.txt') > 1:\n",
    "    # Read the scala-end-results.txt file\n",
    "    df1 = pd.read_csv('scala-end-results.txt', header=None, sep='.txt:', engine='python')\n",
    "    # Give column names\n",
    "    df1.columns = ['Test_group','Results']\n",
    "    # Iliminate the path to SparkTestSuite file and keep only the module name \n",
    "    df1['Test_group'] = df1['Test_group'].str.replace(\"/target/surefire-reports/SparkTestSuite\",\"\")\n",
    "    # Iliminate all strings in Results column\n",
    "    df1['Results'] = df1['Results'].str.replace(\"Tests: succeeded\",\"\").str.replace(\"failed\",\"\").str.replace(\"canceled\",\"\").str.replace(\"ignored\",\"\").str.replace(\"pending\",\"\")\n",
    "    # Split the Results column into sevral columns\n",
    "    splitted_columns = df1['Results'].str.split(',', expand=True)\n",
    "    # Give column names\n",
    "    splitted_columns.columns = ['Succeeded', 'Failed', 'Canceled', 'Ignored', 'Pending']\n",
    "    # Transform string to integers\n",
    "    splitted_columns = splitted_columns.astype(int)\n",
    "    # Concatinate new df\n",
    "    df1 = pd.concat([df1, splitted_columns], axis = 1).drop(columns = ['Results'])\n",
    "    # Join the columns Canceled and Ignored in one signle coulumn\n",
    "    df1['Skipped'] = df1['Canceled'] + df1['Ignored']\n",
    "    # Drop the columns which are not needed anymore\n",
    "    df1 = df1.drop(columns = ['Canceled', 'Ignored'])\n",
    "    # Reorder the columns\n",
    "    df1 = df1[['Test_group', 'Succeeded', 'Failed', 'Skipped', 'Pending']]\n",
    "else:\n",
    "    # Exit since no test has run\n",
    "    columns = ['Test_group', 'Succeeded', 'Failed', 'Skipped', 'Pending']\n",
    "    df1 = pd.DataFrame(columns = columns)\n",
    "    print(\"No module has run\")\n",
    "\n",
    "# Read the aborted-tests.txt file if it has some input in it\n",
    "if os.path.getsize('aborted-tests.txt') > 1:\n",
    "    df2 = pd.read_csv('aborted-tests.txt', header=None, sep='.txt:', engine='python')\n",
    "    # Give column names\n",
    "    df2.columns = ['Test_group','Aborted_tests']\n",
    "    # Iliminate the path to SparkTestSuite file and keep only the module name \n",
    "    df2['Test_group'] = df2['Test_group'].str.replace(\"/target/surefire-reports/SparkTestSuite\",\"\")\n",
    "    # Merge df1 and df2\n",
    "    df1 = pd.concat([df1, df2] , ignore_index=True)\n",
    "else:\n",
    "    # Give the dataframe the column Aborted_tests with empty values\n",
    "    df1[\"Aborted_tests\"] = None\n",
    "    print(\"No aborted tests\")\n",
    "\n",
    "# Read the scala-tests.txt file if it has some input in it\n",
    "if os.path.getsize('scala-tests.txt') > 1:\n",
    "    df3 = pd.read_csv('scala-tests.txt', header=None, sep='.txt:', engine='python')\n",
    "    # Give column names\n",
    "    df3.columns = ['Test_group','Failed_test']\n",
    "    # Iliminate the path to SparkTestSuite file and keep only the module name \n",
    "    df3['Test_group'] = df3['Test_group'].str.replace(\"/target/surefire-reports/SparkTestSuite\",\"\")\n",
    "    # Merge new df1 and df3\n",
    "    df1 = pd.merge(df1, df3, how='outer')\n",
    "else:\n",
    "    # Give the dataframe the column Failed_test with empty colmun\n",
    "    df1[\"Failed_test\"] = None\n",
    "    print(\"No scala test errors\")\n",
    "\n",
    "# Merge df and df1\n",
    "#df = pd.concat([df, df1] , ignore_index=True)\n",
    "\n",
    "# Read existing JSON data\n",
    "with open('testing.json', 'r') as json_file:\n",
    "    existing_data = json.load(json_file)\n",
    "\n",
    "\n",
    "# Update existing data with new data\n",
    "test_group = df1['Test_group']\n",
    "if test_group not in existing_data:\n",
    "    existing_data[test_group] = {\n",
    "        'attributes': {\n",
    "            'Succeeded': df1['Succeeded'],\n",
    "            'Failed': df1['Failed'],\n",
    "            'Skipped': df1['Skipped'],\n",
    "            'Pending': df1['Pending'],\n",
    "            'Aborted_tests': df1['Aborted_tests']\n",
    "        },\n",
    "        'Failed_tests': []\n",
    "    }\n",
    "existing_data[test_group]['Failed_tests'] += df1['Failed_test']\n",
    "\n",
    "# Write the nested dictionary to a JSON file\n",
    "with open(f'results-{build_number}.json', 'w') as json_file:\n",
    "    json.dump(existing_data, json_file, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Test_group</th>\n",
       "      <th>Succeeded</th>\n",
       "      <th>Failed</th>\n",
       "      <th>Skipped</th>\n",
       "      <th>Pending</th>\n",
       "      <th>Aborted_tests</th>\n",
       "      <th>Failed_test</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>org.apache.spark.util.kvstore.InMemoryStoreS...</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[nan]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>org.apache.spark.util.kvstore.ArrayWrappersS...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[nan]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>org.apache.spark.util.kvstore.LevelDBIterato...</td>\n",
       "      <td>38.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[nan]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>org.apache.spark.util.kvstore.InMemoryIterat...</td>\n",
       "      <td>38.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[nan]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>org.apache.spark.util.kvstore.LevelDBTypeInf...</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[nan]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162</th>\n",
       "      <td>org.apache.hadoop.hdfs.TestFileChecksum</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[testStripedFileChecksumWithMissedDataBlocksRa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163</th>\n",
       "      <td>org.apache.hadoop.hdfs.TestFileChecksumComposi...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[testStripedFileChecksumWithMissedDataBlocksRa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>164</th>\n",
       "      <td>org.apache.hadoop.hdfs.TestReconstructStripedF...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[testRecoverAnyBlocks1, testRecoverOneDataBloc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165</th>\n",
       "      <td>org.apache.hadoop.mapred.TestMRTimelineEventHa...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[testMRNewTimelineServiceEventHandling, testMR...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>166</th>\n",
       "      <td>nodeFile null</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[testSimulatorRunning[Testing with: SYNTH, org...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>167 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Test_group  Succeeded  Failed  \\\n",
       "0      org.apache.spark.util.kvstore.InMemoryStoreS...        6.0     0.0   \n",
       "1      org.apache.spark.util.kvstore.ArrayWrappersS...        1.0     0.0   \n",
       "2      org.apache.spark.util.kvstore.LevelDBIterato...       38.0     0.0   \n",
       "3      org.apache.spark.util.kvstore.InMemoryIterat...       38.0     0.0   \n",
       "4      org.apache.spark.util.kvstore.LevelDBTypeInf...       10.0     0.0   \n",
       "..                                                 ...        ...     ...   \n",
       "162            org.apache.hadoop.hdfs.TestFileChecksum        NaN     NaN   \n",
       "163  org.apache.hadoop.hdfs.TestFileChecksumComposi...        NaN     NaN   \n",
       "164  org.apache.hadoop.hdfs.TestReconstructStripedF...        NaN     NaN   \n",
       "165  org.apache.hadoop.mapred.TestMRTimelineEventHa...        NaN     NaN   \n",
       "166                                      nodeFile null        NaN     NaN   \n",
       "\n",
       "     Skipped  Pending  Aborted_tests  \\\n",
       "0        0.0      NaN            NaN   \n",
       "1        0.0      NaN            NaN   \n",
       "2        0.0      NaN            NaN   \n",
       "3        0.0      NaN            NaN   \n",
       "4        0.0      NaN            NaN   \n",
       "..       ...      ...            ...   \n",
       "162      NaN      NaN            NaN   \n",
       "163      NaN      NaN            NaN   \n",
       "164      NaN      NaN            NaN   \n",
       "165      NaN      NaN            NaN   \n",
       "166      NaN      NaN            NaN   \n",
       "\n",
       "                                           Failed_test  \n",
       "0                                                [nan]  \n",
       "1                                                [nan]  \n",
       "2                                                [nan]  \n",
       "3                                                [nan]  \n",
       "4                                                [nan]  \n",
       "..                                                 ...  \n",
       "162  [testStripedFileChecksumWithMissedDataBlocksRa...  \n",
       "163  [testStripedFileChecksumWithMissedDataBlocksRa...  \n",
       "164  [testRecoverAnyBlocks1, testRecoverOneDataBloc...  \n",
       "165  [testMRNewTimelineServiceEventHandling, testMR...  \n",
       "166  [testSimulatorRunning[Testing with: SYNTH, org...  \n",
       "\n",
       "[167 rows x 7 columns]"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read the JSON file\n",
    "with open('testing.json', 'r') as json_file:\n",
    "    json_data = json.load(json_file)\n",
    "\n",
    "# Convert the nested dictionary back into a DataFrame\n",
    "records = []\n",
    "for test_group, values in json_data.items():\n",
    "    record = {'Test_group': test_group}\n",
    "    record.update(values['attributes'])\n",
    "    record['Failed_test'] = values['Failed_tests']\n",
    "    records.append(record)\n",
    "\n",
    "df = pd.DataFrame(records)\n",
    "#df['Failed_test'] = df['Failed_test'].apply(lambda x: x[0])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Test_group</th>\n",
       "      <th>Succeeded</th>\n",
       "      <th>Failed</th>\n",
       "      <th>Skipped</th>\n",
       "      <th>Pending</th>\n",
       "      <th>Aborted_tests</th>\n",
       "      <th>Failed_test</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>org.apache.spark.util.kvstore.InMemoryStoreS...</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[nan]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>org.apache.spark.util.kvstore.ArrayWrappersS...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[nan]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>org.apache.spark.util.kvstore.LevelDBIterato...</td>\n",
       "      <td>38.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[nan]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>org.apache.spark.util.kvstore.InMemoryIterat...</td>\n",
       "      <td>38.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[nan]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>org.apache.spark.util.kvstore.LevelDBTypeInf...</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[nan]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189</th>\n",
       "      <td>external/kafka-0-10</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[nan]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190</th>\n",
       "      <td>sql/catalyst</td>\n",
       "      <td>2611.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[- daysToMillis and millisToDays *** FAILED ***]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191</th>\n",
       "      <td>sql/core</td>\n",
       "      <td>4109.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>59.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[- SPARK-6330 regression test *** FAILED ***]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192</th>\n",
       "      <td>sql/hive</td>\n",
       "      <td>2344.0</td>\n",
       "      <td>510.0</td>\n",
       "      <td>595.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[- SPARK-22745 - read Hive's statistics for pa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193</th>\n",
       "      <td>sql/hive-thriftserver</td>\n",
       "      <td>40.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[nan]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>194 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Test_group  Succeeded  Failed  \\\n",
       "0      org.apache.spark.util.kvstore.InMemoryStoreS...        6.0     0.0   \n",
       "1      org.apache.spark.util.kvstore.ArrayWrappersS...        1.0     0.0   \n",
       "2      org.apache.spark.util.kvstore.LevelDBIterato...       38.0     0.0   \n",
       "3      org.apache.spark.util.kvstore.InMemoryIterat...       38.0     0.0   \n",
       "4      org.apache.spark.util.kvstore.LevelDBTypeInf...       10.0     0.0   \n",
       "..                                                 ...        ...     ...   \n",
       "189                                external/kafka-0-10       15.0     0.0   \n",
       "190                                       sql/catalyst     2611.0     1.0   \n",
       "191                                           sql/core     4109.0     1.0   \n",
       "192                                           sql/hive     2344.0   510.0   \n",
       "193                              sql/hive-thriftserver       40.0     0.0   \n",
       "\n",
       "     Skipped  Pending  Aborted_tests  \\\n",
       "0        0.0      NaN            NaN   \n",
       "1        0.0      NaN            NaN   \n",
       "2        0.0      NaN            NaN   \n",
       "3        0.0      NaN            NaN   \n",
       "4        0.0      NaN            NaN   \n",
       "..       ...      ...            ...   \n",
       "189      0.0      0.0            NaN   \n",
       "190      2.0      0.0            NaN   \n",
       "191     59.0      0.0            NaN   \n",
       "192    595.0      0.0            NaN   \n",
       "193      2.0      0.0            NaN   \n",
       "\n",
       "                                           Failed_test  \n",
       "0                                                [nan]  \n",
       "1                                                [nan]  \n",
       "2                                                [nan]  \n",
       "3                                                [nan]  \n",
       "4                                                [nan]  \n",
       "..                                                 ...  \n",
       "189                                              [nan]  \n",
       "190   [- daysToMillis and millisToDays *** FAILED ***]  \n",
       "191      [- SPARK-6330 regression test *** FAILED ***]  \n",
       "192  [- SPARK-22745 - read Hive's statistics for pa...  \n",
       "193                                              [nan]  \n",
       "\n",
       "[194 rows x 7 columns]"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comparison_run = '0'\n",
    "with open(f'results-{10}.json', 'r') as json_file:\n",
    "            json_data = json.load(json_file)\n",
    "\n",
    "# Convert the nested dictionary into a DataFrame\n",
    "# Create a list called record\n",
    "records = []\n",
    "# Loop through each element of the json file\n",
    "for test_group, values in json_data.items():\n",
    "    # Get the class Test_group and define it as a variable test_group\n",
    "    record = {'Test_group': test_group}\n",
    "    # Get the attribute values\n",
    "    record.update(values['attributes'])\n",
    "    # Get the list of failed tests\n",
    "    record['Failed_test'] = values['Failed_tests']\n",
    "    # Append all these elements to the list records\n",
    "    records.append(record)\n",
    "\n",
    "# Define the dataframe\n",
    "df = pd.DataFrame(records)\n",
    "\n",
    "# Load the external dataframe which we want to compare\n",
    "# If We do not give a comparison run, we compare it with the same dataset which will not give any difference\n",
    "if comparison_run == \"0\":\n",
    "    df_external = df\n",
    "# Otherwise we compare it with the dataset we give for comparison\n",
    "else:\n",
    "    # Read the JSON data as a dictionary\n",
    "    #with open(f'{comparison_run}', 'r') as json_file:\n",
    "        #json_data = json.load(json_file)\n",
    "    #df = pd.read_json(f'results-{comparison_run}.json', orient= \"table\")\n",
    "\n",
    "    # Read the JSON data as a dictionary\n",
    "    with open(f'{comparison_run}', 'r') as json_file:\n",
    "        json_data = json.load(json_file)\n",
    "\n",
    "    # Convert the nested dictionary into a DataFrame\n",
    "    # Create a list called record\n",
    "    records = []\n",
    "    # Loop through each element of the json file\n",
    "    for test_group, values in json_data.items():\n",
    "        # Get the class Test_group and define it as a variable test_group\n",
    "        record = {'Test_group': test_group}\n",
    "        # Get the attribute values\n",
    "        record.update(values['attributes'])\n",
    "        # Get the list of failed tests\n",
    "        record['Failed_test'] = values['Failed_tests']\n",
    "        # Append all these elements to the list records\n",
    "        records.append(record)\n",
    "\n",
    "# Define the dataframe\n",
    "df_external = pd.DataFrame(records)\n",
    "\n",
    "df\n",
    "# Merge ancient and new tables\n",
    "#df = df.merge(df_external, how='left', indicator=True)\n",
    "# Checks if there is an aborted test or a test error in the new table\n",
    "#df[(df['_merge'] == 'left_only')]\n",
    "#df_comparison = df[((df['_merge'] == 'left_only') & pd.notna(df['Aborted_tests'])) | ((df['_merge'] == 'left_only') & pd.notna(df['Failed_test']))]\n",
    "#print(\"Comparison succeeded\")\n",
    "#print(df_comparison)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Test_group</th>\n",
       "      <th>Succeeded</th>\n",
       "      <th>Failed</th>\n",
       "      <th>Skipped</th>\n",
       "      <th>Pending</th>\n",
       "      <th>Aborted_tests</th>\n",
       "      <th>Failed_test</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>org.apache.spark.util.kvstore.InMemoryStoreS...</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>org.apache.spark.util.kvstore.ArrayWrappersS...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>org.apache.spark.util.kvstore.LevelDBIterato...</td>\n",
       "      <td>38.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>org.apache.spark.util.kvstore.InMemoryIterat...</td>\n",
       "      <td>38.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>org.apache.spark.util.kvstore.LevelDBTypeInf...</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249</th>\n",
       "      <td>org.apache.hadoop.mapred.TestMRTimelineEventHa...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>testMRNewTimelineServiceEventHandling</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>250</th>\n",
       "      <td>org.apache.hadoop.mapred.TestMRTimelineEventHa...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>testMRNewTimelineServiceEventHandling</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>251</th>\n",
       "      <td>org.apache.hadoop.mapred.TestMRTimelineEventHa...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>testMRNewTimelineServiceEventHandling</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>252</th>\n",
       "      <td>org.apache.hadoop.mapred.TestMRTimelineEventHa...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>testMRNewTimelineServiceEventHandling</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>253</th>\n",
       "      <td>nodeFile null</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>testSimulatorRunning[Testing with: SYNTH, org....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>254 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Test_group  Succeeded  Failed  \\\n",
       "0      org.apache.spark.util.kvstore.InMemoryStoreS...        6.0     0.0   \n",
       "1      org.apache.spark.util.kvstore.ArrayWrappersS...        1.0     0.0   \n",
       "2      org.apache.spark.util.kvstore.LevelDBIterato...       38.0     0.0   \n",
       "3      org.apache.spark.util.kvstore.InMemoryIterat...       38.0     0.0   \n",
       "4      org.apache.spark.util.kvstore.LevelDBTypeInf...       10.0     0.0   \n",
       "..                                                 ...        ...     ...   \n",
       "249  org.apache.hadoop.mapred.TestMRTimelineEventHa...        NaN     NaN   \n",
       "250  org.apache.hadoop.mapred.TestMRTimelineEventHa...        NaN     NaN   \n",
       "251  org.apache.hadoop.mapred.TestMRTimelineEventHa...        NaN     NaN   \n",
       "252  org.apache.hadoop.mapred.TestMRTimelineEventHa...        NaN     NaN   \n",
       "253                                      nodeFile null        NaN     NaN   \n",
       "\n",
       "     Skipped  Pending  Aborted_tests  \\\n",
       "0        0.0      NaN            NaN   \n",
       "1        0.0      NaN            NaN   \n",
       "2        0.0      NaN            NaN   \n",
       "3        0.0      NaN            NaN   \n",
       "4        0.0      NaN            NaN   \n",
       "..       ...      ...            ...   \n",
       "249      NaN      NaN            NaN   \n",
       "250      NaN      NaN            NaN   \n",
       "251      NaN      NaN            NaN   \n",
       "252      NaN      NaN            NaN   \n",
       "253      NaN      NaN            NaN   \n",
       "\n",
       "                                           Failed_test  \n",
       "0                                                  NaN  \n",
       "1                                                  NaN  \n",
       "2                                                  NaN  \n",
       "3                                                  NaN  \n",
       "4                                                  NaN  \n",
       "..                                                 ...  \n",
       "249              testMRNewTimelineServiceEventHandling  \n",
       "250              testMRNewTimelineServiceEventHandling  \n",
       "251              testMRNewTimelineServiceEventHandling  \n",
       "252              testMRNewTimelineServiceEventHandling  \n",
       "253  testSimulatorRunning[Testing with: SYNTH, org....  \n",
       "\n",
       "[254 rows x 7 columns]"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('testing.json', 'r') as json_file:\n",
    "    json_data = json.load(json_file)\n",
    "\n",
    "# Create a list of dictionaries for each row\n",
    "records = []\n",
    "for group, data in json_data.items():\n",
    "    attributes = data['attributes']\n",
    "    for failed_test in data['Failed_tests']:\n",
    "        record = {'Test_group': group}\n",
    "        record.update(attributes)\n",
    "        record['Failed_test'] = failed_test\n",
    "        records.append(record)\n",
    "\n",
    "# Create a DataFrame from the list of dictionaries\n",
    "df = pd.DataFrame(records)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "object of type 'float' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/stephan/Documents/github/tdp-alliage-jenkins/spark/test_comparison/test.ipynb Cell 23\u001b[0m in \u001b[0;36m<cell line: 25>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/stephan/Documents/github/tdp-alliage-jenkins/spark/test_comparison/test.ipynb#X32sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mDataFrame(records)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/stephan/Documents/github/tdp-alliage-jenkins/spark/test_comparison/test.ipynb#X32sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m         \u001b[39m# Remove the list element of each value in Failed tests\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/stephan/Documents/github/tdp-alliage-jenkins/spark/test_comparison/test.ipynb#X32sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m df[\u001b[39m'\u001b[39m\u001b[39mFailed_test\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m df[\u001b[39m'\u001b[39m\u001b[39mFailed_test\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mapply(\u001b[39mlambda\u001b[39;00m x: x[\u001b[39m0\u001b[39m] \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(x) \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m \u001b[39melse\u001b[39;00m x)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/stephan/Documents/github/tdp-alliage-jenkins/spark/test_comparison/test.ipynb#X32sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m \u001b[39m# Read the scala-end-results.txt file if it has some input in it\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/stephan/Documents/github/tdp-alliage-jenkins/spark/test_comparison/test.ipynb#X32sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m \u001b[39mif\u001b[39;00m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mgetsize(\u001b[39m'\u001b[39m\u001b[39mscala-end-results.txt\u001b[39m\u001b[39m'\u001b[39m) \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/stephan/Documents/github/tdp-alliage-jenkins/spark/test_comparison/test.ipynb#X32sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m     \u001b[39m# Read the scala-end-results.txt file\u001b[39;00m\n",
      "File \u001b[0;32m/nix/store/7dnlgfzajj2drspcdhpdxnpxygkak7m5-python3.10-pandas-1.4.3/lib/python3.10/site-packages/pandas/core/series.py:4433\u001b[0m, in \u001b[0;36mSeries.apply\u001b[0;34m(self, func, convert_dtype, args, **kwargs)\u001b[0m\n\u001b[1;32m   4323\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mapply\u001b[39m(\n\u001b[1;32m   4324\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m   4325\u001b[0m     func: AggFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4328\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[1;32m   4329\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m DataFrame \u001b[39m|\u001b[39m Series:\n\u001b[1;32m   4330\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   4331\u001b[0m \u001b[39m    Invoke function on values of Series.\u001b[39;00m\n\u001b[1;32m   4332\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4431\u001b[0m \u001b[39m    dtype: float64\u001b[39;00m\n\u001b[1;32m   4432\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 4433\u001b[0m     \u001b[39mreturn\u001b[39;00m SeriesApply(\u001b[39mself\u001b[39;49m, func, convert_dtype, args, kwargs)\u001b[39m.\u001b[39;49mapply()\n",
      "File \u001b[0;32m/nix/store/7dnlgfzajj2drspcdhpdxnpxygkak7m5-python3.10-pandas-1.4.3/lib/python3.10/site-packages/pandas/core/apply.py:1088\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1084\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mf, \u001b[39mstr\u001b[39m):\n\u001b[1;32m   1085\u001b[0m     \u001b[39m# if we are a string, try to dispatch\u001b[39;00m\n\u001b[1;32m   1086\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapply_str()\n\u001b[0;32m-> 1088\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mapply_standard()\n",
      "File \u001b[0;32m/nix/store/7dnlgfzajj2drspcdhpdxnpxygkak7m5-python3.10-pandas-1.4.3/lib/python3.10/site-packages/pandas/core/apply.py:1143\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1137\u001b[0m         values \u001b[39m=\u001b[39m obj\u001b[39m.\u001b[39mastype(\u001b[39mobject\u001b[39m)\u001b[39m.\u001b[39m_values\n\u001b[1;32m   1138\u001b[0m         \u001b[39m# error: Argument 2 to \"map_infer\" has incompatible type\u001b[39;00m\n\u001b[1;32m   1139\u001b[0m         \u001b[39m# \"Union[Callable[..., Any], str, List[Union[Callable[..., Any], str]],\u001b[39;00m\n\u001b[1;32m   1140\u001b[0m         \u001b[39m# Dict[Hashable, Union[Union[Callable[..., Any], str],\u001b[39;00m\n\u001b[1;32m   1141\u001b[0m         \u001b[39m# List[Union[Callable[..., Any], str]]]]]\"; expected\u001b[39;00m\n\u001b[1;32m   1142\u001b[0m         \u001b[39m# \"Callable[[Any], Any]\"\u001b[39;00m\n\u001b[0;32m-> 1143\u001b[0m         mapped \u001b[39m=\u001b[39m lib\u001b[39m.\u001b[39;49mmap_infer(\n\u001b[1;32m   1144\u001b[0m             values,\n\u001b[1;32m   1145\u001b[0m             f,  \u001b[39m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[1;32m   1146\u001b[0m             convert\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconvert_dtype,\n\u001b[1;32m   1147\u001b[0m         )\n\u001b[1;32m   1149\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(mapped) \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(mapped[\u001b[39m0\u001b[39m], ABCSeries):\n\u001b[1;32m   1150\u001b[0m     \u001b[39m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[1;32m   1151\u001b[0m     \u001b[39m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[1;32m   1152\u001b[0m     \u001b[39mreturn\u001b[39;00m obj\u001b[39m.\u001b[39m_constructor_expanddim(\u001b[39mlist\u001b[39m(mapped), index\u001b[39m=\u001b[39mobj\u001b[39m.\u001b[39mindex)\n",
      "File \u001b[0;32m/nix/store/7dnlgfzajj2drspcdhpdxnpxygkak7m5-python3.10-pandas-1.4.3/lib/python3.10/site-packages/pandas/_libs/lib.pyx:2870\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
      "\u001b[1;32m/home/stephan/Documents/github/tdp-alliage-jenkins/spark/test_comparison/test.ipynb Cell 23\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/stephan/Documents/github/tdp-alliage-jenkins/spark/test_comparison/test.ipynb#X32sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mDataFrame(records)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/stephan/Documents/github/tdp-alliage-jenkins/spark/test_comparison/test.ipynb#X32sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m         \u001b[39m# Remove the list element of each value in Failed tests\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/stephan/Documents/github/tdp-alliage-jenkins/spark/test_comparison/test.ipynb#X32sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m df[\u001b[39m'\u001b[39m\u001b[39mFailed_test\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m df[\u001b[39m'\u001b[39m\u001b[39mFailed_test\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mapply(\u001b[39mlambda\u001b[39;00m x: x[\u001b[39m0\u001b[39m] \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39;49m(x) \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m \u001b[39melse\u001b[39;00m x)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/stephan/Documents/github/tdp-alliage-jenkins/spark/test_comparison/test.ipynb#X32sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m \u001b[39m# Read the scala-end-results.txt file if it has some input in it\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/stephan/Documents/github/tdp-alliage-jenkins/spark/test_comparison/test.ipynb#X32sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m \u001b[39mif\u001b[39;00m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mgetsize(\u001b[39m'\u001b[39m\u001b[39mscala-end-results.txt\u001b[39m\u001b[39m'\u001b[39m) \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/stephan/Documents/github/tdp-alliage-jenkins/spark/test_comparison/test.ipynb#X32sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m     \u001b[39m# Read the scala-end-results.txt file\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: object of type 'float' has no len()"
     ]
    }
   ],
   "source": [
    "# Read the output from the java transformer\n",
    "with open('output.json', 'r') as json_file:\n",
    "    json_data = json.load(json_file)\n",
    "\n",
    "# Convert the nested dictionary into a DataFrame\n",
    "# Create a list called record\n",
    "    records = []\n",
    "    # Loop through each element of the json file\n",
    "    for test_group, values in json_data.items():\n",
    "        # For each element in the failed_test list\n",
    "        for failed_test in values['Failed_tests']:\n",
    "            # Get the class Test_group and define it as a variable test_group\n",
    "            record = {'Test_group': test_group}\n",
    "            # Get the attribute values\n",
    "            record.update(values['attributes'])\n",
    "            # Get the list of failed tests\n",
    "            record['Failed_test'] = failed_test\n",
    "            # Append all these elements to the list records\n",
    "            records.append(record)\n",
    "\n",
    "# Define the dataframe\n",
    "df = pd.DataFrame(records)\n",
    "\n",
    "\n",
    "\n",
    "# Read the scala-end-results.txt file if it has some input in it\n",
    "if os.path.getsize('scala-end-results.txt') > 1:\n",
    "    # Read the scala-end-results.txt file\n",
    "    df1 = pd.read_csv('scala-end-results.txt', header=None, sep='.txt:', engine='python')\n",
    "    # Give column names\n",
    "    df1.columns = ['Test_group','Results']\n",
    "    # Iliminate the path to SparkTestSuite file and keep only the module name \n",
    "    df1['Test_group'] = df1['Test_group'].str.replace(\"/target/surefire-reports/SparkTestSuite\",\"\")\n",
    "    # Iliminate all strings in Results column\n",
    "    df1['Results'] = df1['Results'].str.replace(\"Tests: succeeded\",\"\").str.replace(\"failed\",\"\").str.replace(\"canceled\",\"\").str.replace(\"ignored\",\"\").str.replace(\"pending\",\"\")\n",
    "    # Split the Results column into sevral columns\n",
    "    splitted_columns = df1['Results'].str.split(',', expand=True)\n",
    "    # Give column names\n",
    "    splitted_columns.columns = ['Succeeded', 'Failed', 'Canceled', 'Ignored', 'Pending']\n",
    "    # Transform string to integers\n",
    "    splitted_columns = splitted_columns.astype(int)\n",
    "    # Concatinate new df\n",
    "    df1 = pd.concat([df1, splitted_columns], axis = 1).drop(columns = ['Results'])\n",
    "    # Join the columns Canceled and Ignored in one signle coulumn\n",
    "    df1['Skipped'] = df1['Canceled'] + df1['Ignored']\n",
    "    # Drop the columns which are not needed anymore\n",
    "    df1 = df1.drop(columns = ['Canceled', 'Ignored'])\n",
    "    # Reorder the columns\n",
    "    df1 = df1[['Test_group', 'Succeeded', 'Failed', 'Skipped', 'Pending']]\n",
    "else:\n",
    "    # Exit since no test has run\n",
    "    columns = ['Test_group', 'Succeeded', 'Failed', 'Skipped', 'Pending']\n",
    "    df1 = pd.DataFrame(columns = columns)\n",
    "    print(\"No module has run\")\n",
    "\n",
    "# Read the aborted-tests.txt file if it has some input in it\n",
    "if os.path.getsize('aborted-tests.txt') > 1:\n",
    "    df2 = pd.read_csv('aborted-tests.txt', header=None, sep='.txt:', engine='python')\n",
    "    # Give column names\n",
    "    df2.columns = ['Test_group','Aborted_tests']\n",
    "    # Iliminate the path to SparkTestSuite file and keep only the module name \n",
    "    df2['Test_group'] = df2['Test_group'].str.replace(\"/target/surefire-reports/SparkTestSuite\",\"\")\n",
    "    # Merge df1 and df2\n",
    "    df1 = pd.concat([df1, df2] , ignore_index=True)\n",
    "else:\n",
    "    # Give the dataframe the column Aborted_tests with empty values\n",
    "    df1[\"Aborted_tests\"] = None\n",
    "    print(\"No aborted tests\")\n",
    "\n",
    "# Read the scala-tests.txt file if it has some input in it\n",
    "if os.path.getsize('scala-tests.txt') > 1:\n",
    "    df3 = pd.read_csv('scala-tests.txt', header=None, sep='.txt:', engine='python')\n",
    "    # Give column names\n",
    "    df3.columns = ['Test_group','Failed_test']\n",
    "    # Iliminate the path to SparkTestSuite file and keep only the module name \n",
    "    df3['Test_group'] = df3['Test_group'].str.replace(\"/target/surefire-reports/SparkTestSuite\",\"\")\n",
    "    # Merge new df1 and df3\n",
    "    df1 = pd.merge(df1, df3, how='outer')\n",
    "else:\n",
    "    # Give the dataframe the column Failed_test with empty colmun\n",
    "    df1[\"Failed_test\"] = None\n",
    "    print(\"No scala test errors\")\n",
    "\n",
    "# Concatinate dataframe from the java tests with df1\n",
    "df = pd.concat([df, df1] , ignore_index=True)\n",
    "\n",
    "# Create a dictionnary\n",
    "nested_dict = {}\n",
    "# Go through each row\n",
    "for _, row in df.iterrows():\n",
    "    # Define the Test_group which will be a class\n",
    "    test_group = row['Test_group']\n",
    "    # If the Test_group is not already in the dictionnary add it to it with its attributes and the list of failed tests\n",
    "    if test_group not in nested_dict:\n",
    "        nested_dict[test_group] = {\n",
    "            'attributes': {\n",
    "                'Succeeded': row['Succeeded'],\n",
    "                'Failed': row['Failed'],\n",
    "                'Skipped': row['Skipped'],\n",
    "                'Pending': row['Pending'],\n",
    "                'Aborted_tests' : row['Aborted_tests']\n",
    "            },\n",
    "            'Failed_tests': []\n",
    "        }\n",
    "    # Append individual failed tests if 'Failed_tests' is a list\n",
    "    if isinstance(row['Failed_test'], list):\n",
    "        nested_dict[test_group]['Failed_tests'].extend(row['Failed_test'])\n",
    "    else:\n",
    "        nested_dict[test_group]['Failed_tests'].append(row['Failed_test'])\n",
    "\n",
    "# Write the nested dictionary to a JSON file\n",
    "with open(f'results-{build_number}.json', 'w') as json_file:\n",
    "    json.dump(nested_dict, json_file, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'  org.apache.spark.util.kvstore.InMemoryStoreSuite': {'attributes': 'dict', 'Failed_tests': 'list'}, '  org.apache.spark.util.kvstore.ArrayWrappersSuite': {'attributes': 'dict', 'Failed_tests': 'list'}, '  org.apache.spark.util.kvstore.LevelDBIteratorSuite': {'attributes': 'dict', 'Failed_tests': 'list'}, '  org.apache.spark.util.kvstore.InMemoryIteratorSuite': {'attributes': 'dict', 'Failed_tests': 'list'}, '  org.apache.spark.util.kvstore.LevelDBTypeInfoSuite': {'attributes': 'dict', 'Failed_tests': 'list'}, '  org.apache.spark.util.kvstore.LevelDBSuite': {'attributes': 'dict', 'Failed_tests': 'list'}, '  org.apache.spark.network.util.TransportFrameDecoderSuite': {'attributes': 'dict', 'Failed_tests': 'list'}, '  org.apache.spark.network.util.NettyMemoryMetricsSuite': {'attributes': 'dict', 'Failed_tests': 'list'}, '  org.apache.spark.network.util.CryptoUtilsSuite': {'attributes': 'dict', 'Failed_tests': 'list'}, '  org.apache.spark.network.TransportResponseHandlerSuite': {'attributes': 'dict', 'Failed_tests': 'list'}, '  org.apache.spark.network.TransportClientFactorySuite': {'attributes': 'dict', 'Failed_tests': 'list'}, '  org.apache.spark.network.protocol.MessageWithHeaderSuite': {'attributes': 'dict', 'Failed_tests': 'list'}, '  org.apache.spark.network.crypto.AuthEngeSuite': {'attributes': 'dict', 'Failed_tests': 'list'}, '  org.apache.spark.network.crypto.AuthMessagesSuite': {'attributes': 'dict', 'Failed_tests': 'list'}, '  org.apache.spark.network.crypto.AuthIntegrationSuite': {'attributes': 'dict', 'Failed_tests': 'list'}, '  org.apache.spark.network.sasl.SparkSaslSuite': {'attributes': 'dict', 'Failed_tests': 'list'}, '  org.apache.spark.network.server.OneForOneStreamManagerSuite': {'attributes': 'dict', 'Failed_tests': 'list'}, '  org.apache.spark.network.ProtocolSuite': {'attributes': 'dict', 'Failed_tests': 'list'}, '  org.apache.spark.network.TransportRequestHandlerSuite': {'attributes': 'dict', 'Failed_tests': 'list'}, '  org.apache.spark.network.RequestTimeoutIntegrationSuite': {'attributes': 'dict', 'Failed_tests': 'list'}, '  org.apache.spark.network.StreamSuite': {'attributes': 'dict', 'Failed_tests': 'list'}, '  org.apache.spark.network.ChunkFetchIntegrationSuite': {'attributes': 'dict', 'Failed_tests': 'list'}, '  org.apache.spark.network.RpcIntegrationSuite': {'attributes': 'dict', 'Failed_tests': 'list'}, '  org.apache.spark.network.sasl.SaslIntegrationSuite': {'attributes': 'dict', 'Failed_tests': 'list'}, '  org.apache.spark.network.sasl.ShuffleSecretManagerSuite': {'attributes': 'dict', 'Failed_tests': 'list'}, '  org.apache.spark.network.shuffle.RetrygBlockFetcherSuite': {'attributes': 'dict', 'Failed_tests': 'list'}, '  org.apache.spark.network.shuffle.ExternalShuffleSecuritySuite': {'attributes': 'dict', 'Failed_tests': 'list'}, '  org.apache.spark.network.shuffle.ExternalShuffleBlockHandlerSuite': {'attributes': 'dict', 'Failed_tests': 'list'}, '  org.apache.spark.network.shuffle.ExternalShuffleIntegrationSuite': {'attributes': 'dict', 'Failed_tests': 'list'}, '  org.apache.spark.network.shuffle.ExternalShuffleBlockResolverSuite': {'attributes': 'dict', 'Failed_tests': 'list'}, '  org.apache.spark.network.shuffle.OneForOneBlockFetcherSuite': {'attributes': 'dict', 'Failed_tests': 'list'}, '  org.apache.spark.network.shuffle.ExternalShuffleCleanupSuite': {'attributes': 'dict', 'Failed_tests': 'list'}, '  org.apache.spark.network.shuffle.BlockTransferMessagesSuite': {'attributes': 'dict', 'Failed_tests': 'list'}, '  org.apache.spark.unsafe.types.UTF8StrgSuite': {'attributes': 'dict', 'Failed_tests': 'list'}, '  org.apache.spark.unsafe.types.CalendarIntervalSuite': {'attributes': 'dict', 'Failed_tests': 'list'}, '  org.apache.spark.unsafe.array.LongArraySuite': {'attributes': 'dict', 'Failed_tests': 'list'}, '  org.apache.spark.unsafe.PlatformUtilSuite': {'attributes': 'dict', 'Failed_tests': 'list'}, '  org.apache.spark.unsafe.hash.Murmur3_x86_32Suite': {'attributes': 'dict', 'Failed_tests': 'list'}, '  org.apache.spark.launcher.InProcessLauncherSuite': {'attributes': 'dict', 'Failed_tests': 'list'}, '  org.apache.spark.launcher.LauncherServerSuite': {'attributes': 'dict', 'Failed_tests': 'list'}, '  org.apache.spark.launcher.CommandBuilderUtilsSuite': {'attributes': 'dict', 'Failed_tests': 'list'}, '  org.apache.spark.launcher.SparkSubmitCommandBuilderSuite': {'attributes': 'dict', 'Failed_tests': 'list'}, '  org.apache.spark.launcher.ChildProcAppHandleSuite': {'attributes': 'dict', 'Failed_tests': 'list'}, '  org.apache.spark.launcher.SparkSubmitOptionParserSuite': {'attributes': 'dict', 'Failed_tests': 'list'}, '  org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorterRadixSortSuite': {'attributes': 'dict', 'Failed_tests': 'list'}, '  org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorterSuite': {'attributes': 'dict', 'Failed_tests': 'list'}, '  org.apache.spark.util.collection.unsafe.sort.UnsafeInMemorySorterSuite': {'attributes': 'dict', 'Failed_tests': 'list'}, '  org.apache.spark.util.collection.unsafe.sort.UnsafeInMemorySorterRadixSortSuite': {'attributes': 'dict', 'Failed_tests': 'list'}, '  org.apache.spark.api.java.OptionalSuite': {'attributes': 'dict', 'Failed_tests': 'list'}, '  org.apache.spark.io.ReadAheadInputStreamSuite': {'attributes': 'dict', 'Failed_tests': 'list'}, '  org.apache.spark.io.NioBufferedInputStreamSuite': {'attributes': 'dict', 'Failed_tests': 'list'}, '  org.apache.spark.unsafe.map.BytesToBytesMapOffHeapSuite': {'attributes': 'dict', 'Failed_tests': 'list'}, '  org.apache.spark.unsafe.map.BytesToBytesMapOnHeapSuite': {'attributes': 'dict', 'Failed_tests': 'list'}, '  org.apache.spark.shuffle.sort.PackedRecordPoterSuite': {'attributes': 'dict', 'Failed_tests': 'list'}, '  org.apache.spark.shuffle.sort.ShuffleInMemoryRadixSorterSuite': {'attributes': 'dict', 'Failed_tests': 'list'}, '  org.apache.spark.shuffle.sort.ShuffleInMemorySorterSuite': {'attributes': 'dict', 'Failed_tests': 'list'}, '  org.apache.spark.shuffle.sort.UnsafeShuffleWriterSuite': {'attributes': 'dict', 'Failed_tests': 'list'}, '  org.apache.spark.memory.TaskMemoryManagerSuite': {'attributes': 'dict', 'Failed_tests': 'list'}, '  org.apache.spark.JavaJdbcRDDSuite': {'attributes': 'dict', 'Failed_tests': 'list'}, '  org.apache.spark.launcher.SparkLauncherSuite': {'attributes': 'dict', 'Failed_tests': 'list'}, '  test.org.apache.spark.Java8RDDAPISuite': {'attributes': 'dict', 'Failed_tests': 'list'}, '  test.org.apache.spark.JavaAPISuite': {'attributes': 'dict', 'Failed_tests': 'list'}, '  test.org.apache.spark.JavaSparkContextSuite': {'attributes': 'dict', 'Failed_tests': 'list'}, '  org.apache.spark.streamg.JavaDurationSuite': {'attributes': 'dict', 'Failed_tests': 'list'}, '  org.apache.spark.streamg.JavaWriteAheadLogSuite': {'attributes': 'dict', 'Failed_tests': 'list'}, '  org.apache.spark.streamg.JavaMapWithStateSuite': {'attributes': 'dict', 'Failed_tests': 'list'}, '  org.apache.spark.streamg.JavaReceiverAPISuite': {'attributes': 'dict', 'Failed_tests': 'list'}, '  org.apache.spark.streamg.JavaTimeSuite': {'attributes': 'dict', 'Failed_tests': 'list'}, '  test.org.apache.spark.streamg.Java8APISuite': {'attributes': 'dict', 'Failed_tests': 'list'}, '  test.org.apache.spark.streamg.JavaAPISuite': {'attributes': 'dict', 'Failed_tests': 'list'}, '  org.apache.spark.sql.streamg.JavaOutputModeSuite': {'attributes': 'dict', 'Failed_tests': 'list'}, '  org.apache.spark.sql.streamg.JavaGroupStateTimeoutSuite': {'attributes': 'dict', 'Failed_tests': 'list'}, '  org.apache.spark.sql.catalyst.expressions.XXH64Suite': {'attributes': 'dict', 'Failed_tests': 'list'}, '  org.apache.spark.sql.catalyst.expressions.RowBasedKeyValueBatchSuite': {'attributes': 'dict', 'Failed_tests': 'list'}, '  org.apache.spark.sql.catalyst.expressions.HiveHasherSuite': {'attributes': 'dict', 'Failed_tests': 'list'}, '  test.org.apache.spark.sql.JavaDatasetAggregatorSuite': {'attributes': 'dict', 'Failed_tests': 'list'}, '  test.org.apache.spark.sql.execution.sort.RecordBaryComparatorSuite': {'attributes': 'dict', 'Failed_tests': 'list'}, '  test.org.apache.spark.sql.JavaDatasetSuite': {'attributes': 'dict', 'Failed_tests': 'list'}, '  test.org.apache.spark.sql.JavaBeanWithArraySuite': {'attributes': 'dict', 'Failed_tests': 'list'}, '  test.org.apache.spark.sql.JavaUDAFSuite': {'attributes': 'dict', 'Failed_tests': 'list'}, '  test.org.apache.spark.sql.JavaDataFrameSuite': {'attributes': 'dict', 'Failed_tests': 'list'}, '  test.org.apache.spark.sql.JavaDataFrameReaderWriterSuite': {'attributes': 'dict', 'Failed_tests': 'list'}, '  test.org.apache.spark.sql.JavaUDFSuite': {'attributes': 'dict', 'Failed_tests': 'list'}, '  test.org.apache.spark.sql.JavaSaveLoadSuite': {'attributes': 'dict', 'Failed_tests': 'list'}, '  test.org.apache.spark.sql.JavaRowSuite': {'attributes': 'dict', 'Failed_tests': 'list'}, '  test.org.apache.spark.sql.JavaApplySchemaSuite': {'attributes': 'dict', 'Failed_tests': 'list'}, '  test.org.apache.spark.sql.Java8DatasetAggregatorSuite': {'attributes': 'dict', 'Failed_tests': 'list'}, '  org.apache.spark.mllib.stat.JavaStatisticsSuite': {'attributes': 'dict', 'Failed_tests': 'list'}, '  org.apache.spark.mllib.util.JavaMLUtilsSuite': {'attributes': 'dict', 'Failed_tests': 'list'}, '  org.apache.spark.mllib.fpm.JavaAssociationRulesSuite': {'attributes': 'dict', 'Failed_tests': 'list'}, '  org.apache.spark.mllib.fpm.JavaPrefixSpanSuite': {'attributes': 'dict', 'Failed_tests': 'list'}, '  org.apache.spark.mllib.fpm.JavaFPGrowthSuite': {'attributes': 'dict', 'Failed_tests': 'list'}, '  org.apache.spark.mllib.feature.JavaWord2VecSuite': {'attributes': 'dict', 'Failed_tests': 'list'}, '  org.apache.spark.mllib.feature.JavaTfIdfSuite': {'attributes': 'dict', 'Failed_tests': 'list'}, '  org.apache.spark.mllib.clusterg.JavaStreamgKMeansSuite': {'attributes': 'dict', 'Failed_tests': 'list'}, '  org.apache.spark.mllib.clusterg.JavaKMeansSuite': {'attributes': 'dict', 'Failed_tests': 'list'}, '  org.apache.spark.mllib.clusterg.JavaLDASuite': {'attributes': 'dict', 'Failed_tests': 'list'}, '  org.apache.spark.mllib.clusterg.JavaGaussianMixtureSuite': {'attributes': 'dict', 'Failed_tests': 'list'}, '  org.apache.spark.mllib.clusterg.JavaBisectgKMeansSuite': {'attributes': 'dict', 'Failed_tests': 'list'}, '  org.apache.spark.mllib.evaluation.JavaRankgMetricsSuite': {'attributes': 'dict', 'Failed_tests': 'list'}, '  org.apache.spark.mllib.recommendation.JavaALSSuite': {'attributes': 'dict', 'Failed_tests': 'list'}, '  org.apache.spark.mllib.regression.JavaStreamgLearRegressionSuite': {'attributes': 'dict', 'Failed_tests': 'list'}, '  org.apache.spark.mllib.regression.JavaLearRegressionSuite': {'attributes': 'dict', 'Failed_tests': 'list'}, '  org.apache.spark.mllib.regression.JavaLassoSuite': {'attributes': 'dict', 'Failed_tests': 'list'}, '  org.apache.spark.mllib.regression.JavaRidgeRegressionSuite': {'attributes': 'dict', 'Failed_tests': 'list'}, '  org.apache.spark.mllib.regression.JavaIsotonicRegressionSuite': {'attributes': 'dict', 'Failed_tests': 'list'}, '  org.apache.spark.mllib.lalg.JavaMatricesSuite': {'attributes': 'dict', 'Failed_tests': 'list'}, '  org.apache.spark.mllib.lalg.JavaVectorsSuite': {'attributes': 'dict', 'Failed_tests': 'list'}, '  org.apache.spark.mllib.lalg.distributed.JavaRowMatrixSuite': {'attributes': 'dict', 'Failed_tests': 'list'}, '  org.apache.spark.mllib.tree.JavaDecisionTreeSuite': {'attributes': 'dict', 'Failed_tests': 'list'}, '  org.apache.spark.mllib.classification.JavaStreamgLogisticRegressionSuite': {'attributes': 'dict', 'Failed_tests': 'list'}, '  org.apache.spark.mllib.classification.JavaNaiveBayesSuite': {'attributes': 'dict', 'Failed_tests': 'list'}, '  org.apache.spark.mllib.classification.JavaSVMSuite': {'attributes': 'dict', 'Failed_tests': 'list'}, '  org.apache.spark.mllib.classification.JavaLogisticRegressionSuite': {'attributes': 'dict', 'Failed_tests': 'list'}, '  org.apache.spark.mllib.random.JavaRandomRDDsSuite': {'attributes': 'dict', 'Failed_tests': 'list'}, '  org.apache.spark.ml.stat.JavaSummarizerSuite': {'attributes': 'dict', 'Failed_tests': 'list'}, '  org.apache.spark.ml.util.JavaDefaultReadWriteSuite': {'attributes': 'dict', 'Failed_tests': 'list'}, '  org.apache.spark.ml.param.JavaParamsSuite': {'attributes': 'dict', 'Failed_tests': 'list'}, '  org.apache.spark.ml.feature.JavaPolynomialExpansionSuite': {'attributes': 'dict', 'Failed_tests': 'list'}, '  org.apache.spark.ml.feature.JavaPCASuite': {'attributes': 'dict', 'Failed_tests': 'list'}, '  org.apache.spark.ml.feature.JavaDCTSuite': {'attributes': 'dict', 'Failed_tests': 'list'}, '  org.apache.spark.ml.feature.JavaBucketizerSuite': {'attributes': 'dict', 'Failed_tests': 'list'}, '  org.apache.spark.ml.feature.JavaTokenizerSuite': {'attributes': 'dict', 'Failed_tests': 'list'}, '  org.apache.spark.ml.feature.JavaStrgIndexerSuite': {'attributes': 'dict', 'Failed_tests': 'list'}, '  org.apache.spark.ml.feature.JavaWord2VecSuite': {'attributes': 'dict', 'Failed_tests': 'list'}, '  org.apache.spark.ml.feature.JavaVectorIndexerSuite': {'attributes': 'dict', 'Failed_tests': 'list'}, '  org.apache.spark.ml.feature.JavaVectorAssemblerSuite': {'attributes': 'dict', 'Failed_tests': 'list'}, '  org.apache.spark.ml.feature.JavaStandardScalerSuite': {'attributes': 'dict', 'Failed_tests': 'list'}, '  org.apache.spark.ml.feature.JavaVectorSlicerSuite': {'attributes': 'dict', 'Failed_tests': 'list'}, '  org.apache.spark.ml.feature.JavaStopWordsRemoverSuite': {'attributes': 'dict', 'Failed_tests': 'list'}, '  org.apache.spark.ml.feature.JavaNormalizerSuite': {'attributes': 'dict', 'Failed_tests': 'list'}, '  org.apache.spark.ml.feature.JavaHashgTFSuite': {'attributes': 'dict', 'Failed_tests': 'list'}, '  org.apache.spark.ml.source.libsvm.JavaLibSVMRelationSuite': {'attributes': 'dict', 'Failed_tests': 'list'}, '  org.apache.spark.ml.clusterg.JavaKMeansSuite': {'attributes': 'dict', 'Failed_tests': 'list'}, '  org.apache.spark.ml.tung.JavaCrossValidatorSuite': {'attributes': 'dict', 'Failed_tests': 'list'}, '  org.apache.spark.ml.JavaPipeleSuite': {'attributes': 'dict', 'Failed_tests': 'list'}, '  org.apache.spark.ml.attribute.JavaAttributeSuite': {'attributes': 'dict', 'Failed_tests': 'list'}, '  org.apache.spark.ml.attribute.JavaAttributeGroupSuite': {'attributes': 'dict', 'Failed_tests': 'list'}, '  org.apache.spark.ml.regression.JavaLearRegressionSuite': {'attributes': 'dict', 'Failed_tests': 'list'}, '  org.apache.spark.ml.regression.JavaRandomForestRegressorSuite': {'attributes': 'dict', 'Failed_tests': 'list'}, '  org.apache.spark.ml.regression.JavaGBTRegressorSuite': {'attributes': 'dict', 'Failed_tests': 'list'}, '  org.apache.spark.ml.regression.JavaDecisionTreeRegressorSuite': {'attributes': 'dict', 'Failed_tests': 'list'}, '  org.apache.spark.ml.lalg.JavaSQLDataTypesSuite': {'attributes': 'dict', 'Failed_tests': 'list'}, '  org.apache.spark.ml.classification.JavaDecisionTreeClassifierSuite': {'attributes': 'dict', 'Failed_tests': 'list'}, '  org.apache.spark.ml.classification.JavaGBTClassifierSuite': {'attributes': 'dict', 'Failed_tests': 'list'}, '  org.apache.spark.ml.classification.JavaNaiveBayesSuite': {'attributes': 'dict', 'Failed_tests': 'list'}, '  org.apache.spark.ml.classification.JavaMultilayerPerceptronClassifierSuite': {'attributes': 'dict', 'Failed_tests': 'list'}, '  org.apache.spark.ml.classification.JavaOneVsRestSuite': {'attributes': 'dict', 'Failed_tests': 'list'}, '  org.apache.spark.ml.classification.JavaLogisticRegressionSuite': {'attributes': 'dict', 'Failed_tests': 'list'}, '  org.apache.spark.ml.classification.JavaRandomForestClassifierSuite': {'attributes': 'dict', 'Failed_tests': 'list'}, '  org.apache.spark.sql.hive.JavaDataFrameSuite': {'attributes': 'dict', 'Failed_tests': 'list'}, '  org.apache.spark.sql.hive.JavaMetastoreDataSourcesSuite': {'attributes': 'dict', 'Failed_tests': 'list'}, '  org.apache.spark.streamg.kafka010.JavaConsumerStrategySuite': {'attributes': 'dict', 'Failed_tests': 'list'}, '  org.apache.spark.streamg.kafka010.JavaLocationStrategySuite': {'attributes': 'dict', 'Failed_tests': 'list'}, '  org.apache.spark.streamg.kafka010.JavaDirectKafkaStreamSuite': {'attributes': 'dict', 'Failed_tests': 'list'}, '  org.apache.spark.streamg.kafka010.JavaKafkaRDDSuite': {'attributes': 'dict', 'Failed_tests': 'list'}, '  org.apache.spark.streamg.flume.JavaFlumeStreamSuite': {'attributes': 'dict', 'Failed_tests': 'list'}, '  org.apache.spark.streamg.flume.JavaFlumePollgStreamSuite': {'attributes': 'dict', 'Failed_tests': 'list'}, 'org.apache.hadoop.util.TestReadWriteDiskValidator': {'attributes': 'dict', 'Failed_tests': 'list'}, 'org.apache.hadoop.io.compress.TestCompressorDecompressor': {'attributes': 'dict', 'Failed_tests': 'list'}, 'org.apache.hadoop.io.compress.snappy.TestSnappyCompressorDecompressor': {'attributes': 'dict', 'Failed_tests': 'list'}, 'org.apache.hadoop.security.TestLdapGroupsMapping': {'attributes': 'dict', 'Failed_tests': 'list'}, 'org.apache.hadoop.hdfs.TestFileChecksum': {'attributes': 'dict', 'Failed_tests': 'list'}, 'org.apache.hadoop.hdfs.TestFileChecksumCompositeCrc': {'attributes': 'dict', 'Failed_tests': 'list'}, 'org.apache.hadoop.hdfs.TestReconstructStripedFileWithRandomECPolicy': {'attributes': 'dict', 'Failed_tests': 'list'}, 'org.apache.hadoop.mapred.TestMRTimelineEventHandling': {'attributes': 'dict', 'Failed_tests': 'list'}, 'nodeFile null': {'attributes': 'dict', 'Failed_tests': 'list'}}\n"
     ]
    }
   ],
   "source": [
    "with open('testing.json', 'r') as json_file:\n",
    "    json_data = json.load(json_file)\n",
    "\n",
    "# Extract and analyze the keys and value types\n",
    "schema = {}\n",
    "for group, data in json_data.items():\n",
    "    schema[group] = {}\n",
    "    for key, value in data.items():\n",
    "        value_type = type(value).__name__\n",
    "        schema[group][key] = value_type\n",
    "\n",
    "print(schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>org.apache.spark.util.kvstore.InMemoryStoreSuite</th>\n",
       "      <th>org.apache.spark.util.kvstore.ArrayWrappersSuite</th>\n",
       "      <th>org.apache.spark.util.kvstore.LevelDBIteratorSuite</th>\n",
       "      <th>org.apache.spark.util.kvstore.InMemoryIteratorSuite</th>\n",
       "      <th>org.apache.spark.util.kvstore.LevelDBTypeInfoSuite</th>\n",
       "      <th>org.apache.spark.util.kvstore.LevelDBSuite</th>\n",
       "      <th>org.apache.spark.network.util.TransportFrameDecoderSuite</th>\n",
       "      <th>org.apache.spark.network.util.NettyMemoryMetricsSuite</th>\n",
       "      <th>org.apache.spark.network.util.CryptoUtilsSuite</th>\n",
       "      <th>org.apache.spark.network.TransportResponseHandlerSuite</th>\n",
       "      <th>...</th>\n",
       "      <th>external/flume-assembly</th>\n",
       "      <th>external/flume-sink</th>\n",
       "      <th>external/flume</th>\n",
       "      <th>external/kafka-0-10-assembly</th>\n",
       "      <th>external/kafka-0-10-sql</th>\n",
       "      <th>external/kafka-0-10</th>\n",
       "      <th>sql/catalyst</th>\n",
       "      <th>sql/core</th>\n",
       "      <th>sql/hive</th>\n",
       "      <th>sql/hive-thriftserver</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>attributes</th>\n",
       "      <td>{'Succeeded': 6.0, 'Failed': 0.0, 'Skipped': 0...</td>\n",
       "      <td>{'Succeeded': 1.0, 'Failed': 0.0, 'Skipped': 0...</td>\n",
       "      <td>{'Succeeded': 38.0, 'Failed': 0.0, 'Skipped': ...</td>\n",
       "      <td>{'Succeeded': 38.0, 'Failed': 0.0, 'Skipped': ...</td>\n",
       "      <td>{'Succeeded': 10.0, 'Failed': 0.0, 'Skipped': ...</td>\n",
       "      <td>{'Succeeded': 8.0, 'Failed': 0.0, 'Skipped': 0...</td>\n",
       "      <td>{'Succeeded': 6.0, 'Failed': 0.0, 'Skipped': 0...</td>\n",
       "      <td>{'Succeeded': 2.0, 'Failed': 0.0, 'Skipped': 0...</td>\n",
       "      <td>{'Succeeded': 1.0, 'Failed': 0.0, 'Skipped': 0...</td>\n",
       "      <td>{'Succeeded': 8.0, 'Failed': 0.0, 'Skipped': 0...</td>\n",
       "      <td>...</td>\n",
       "      <td>{'Succeeded': 0.0, 'Failed': 0.0, 'Skipped': 0...</td>\n",
       "      <td>{'Succeeded': 5.0, 'Failed': 0.0, 'Skipped': 0...</td>\n",
       "      <td>{'Succeeded': 4.0, 'Failed': 0.0, 'Skipped': 0...</td>\n",
       "      <td>{'Succeeded': 0.0, 'Failed': 0.0, 'Skipped': 0...</td>\n",
       "      <td>{'Succeeded': 105.0, 'Failed': 0.0, 'Skipped':...</td>\n",
       "      <td>{'Succeeded': 15.0, 'Failed': 0.0, 'Skipped': ...</td>\n",
       "      <td>{'Succeeded': 2611.0, 'Failed': 1.0, 'Skipped'...</td>\n",
       "      <td>{'Succeeded': 4109.0, 'Failed': 1.0, 'Skipped'...</td>\n",
       "      <td>{'Succeeded': 2344.0, 'Failed': 510.0, 'Skippe...</td>\n",
       "      <td>{'Succeeded': 40.0, 'Failed': 0.0, 'Skipped': ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Failed_tests</th>\n",
       "      <td>[None]</td>\n",
       "      <td>[None]</td>\n",
       "      <td>[None]</td>\n",
       "      <td>[None]</td>\n",
       "      <td>[None]</td>\n",
       "      <td>[None]</td>\n",
       "      <td>[None]</td>\n",
       "      <td>[None]</td>\n",
       "      <td>[None]</td>\n",
       "      <td>[None]</td>\n",
       "      <td>...</td>\n",
       "      <td>[None]</td>\n",
       "      <td>[None]</td>\n",
       "      <td>[None]</td>\n",
       "      <td>[None]</td>\n",
       "      <td>[None]</td>\n",
       "      <td>[None]</td>\n",
       "      <td>[- daysToMillis and millisToDays *** FAILED ***]</td>\n",
       "      <td>[- SPARK-6330 regression test *** FAILED ***]</td>\n",
       "      <td>[- SPARK-22745 - read Hive's statistics for pa...</td>\n",
       "      <td>[None]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 194 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               org.apache.spark.util.kvstore.InMemoryStoreSuite  \\\n",
       "attributes    {'Succeeded': 6.0, 'Failed': 0.0, 'Skipped': 0...   \n",
       "Failed_tests                                             [None]   \n",
       "\n",
       "               org.apache.spark.util.kvstore.ArrayWrappersSuite  \\\n",
       "attributes    {'Succeeded': 1.0, 'Failed': 0.0, 'Skipped': 0...   \n",
       "Failed_tests                                             [None]   \n",
       "\n",
       "               org.apache.spark.util.kvstore.LevelDBIteratorSuite  \\\n",
       "attributes    {'Succeeded': 38.0, 'Failed': 0.0, 'Skipped': ...     \n",
       "Failed_tests                                             [None]     \n",
       "\n",
       "               org.apache.spark.util.kvstore.InMemoryIteratorSuite  \\\n",
       "attributes    {'Succeeded': 38.0, 'Failed': 0.0, 'Skipped': ...      \n",
       "Failed_tests                                             [None]      \n",
       "\n",
       "               org.apache.spark.util.kvstore.LevelDBTypeInfoSuite  \\\n",
       "attributes    {'Succeeded': 10.0, 'Failed': 0.0, 'Skipped': ...     \n",
       "Failed_tests                                             [None]     \n",
       "\n",
       "                     org.apache.spark.util.kvstore.LevelDBSuite  \\\n",
       "attributes    {'Succeeded': 8.0, 'Failed': 0.0, 'Skipped': 0...   \n",
       "Failed_tests                                             [None]   \n",
       "\n",
       "               org.apache.spark.network.util.TransportFrameDecoderSuite  \\\n",
       "attributes    {'Succeeded': 6.0, 'Failed': 0.0, 'Skipped': 0...           \n",
       "Failed_tests                                             [None]           \n",
       "\n",
       "               org.apache.spark.network.util.NettyMemoryMetricsSuite  \\\n",
       "attributes    {'Succeeded': 2.0, 'Failed': 0.0, 'Skipped': 0...        \n",
       "Failed_tests                                             [None]        \n",
       "\n",
       "                 org.apache.spark.network.util.CryptoUtilsSuite  \\\n",
       "attributes    {'Succeeded': 1.0, 'Failed': 0.0, 'Skipped': 0...   \n",
       "Failed_tests                                             [None]   \n",
       "\n",
       "               org.apache.spark.network.TransportResponseHandlerSuite  ...  \\\n",
       "attributes    {'Succeeded': 8.0, 'Failed': 0.0, 'Skipped': 0...        ...   \n",
       "Failed_tests                                             [None]        ...   \n",
       "\n",
       "                                        external/flume-assembly  \\\n",
       "attributes    {'Succeeded': 0.0, 'Failed': 0.0, 'Skipped': 0...   \n",
       "Failed_tests                                             [None]   \n",
       "\n",
       "                                            external/flume-sink  \\\n",
       "attributes    {'Succeeded': 5.0, 'Failed': 0.0, 'Skipped': 0...   \n",
       "Failed_tests                                             [None]   \n",
       "\n",
       "                                                 external/flume  \\\n",
       "attributes    {'Succeeded': 4.0, 'Failed': 0.0, 'Skipped': 0...   \n",
       "Failed_tests                                             [None]   \n",
       "\n",
       "                                   external/kafka-0-10-assembly  \\\n",
       "attributes    {'Succeeded': 0.0, 'Failed': 0.0, 'Skipped': 0...   \n",
       "Failed_tests                                             [None]   \n",
       "\n",
       "                                        external/kafka-0-10-sql  \\\n",
       "attributes    {'Succeeded': 105.0, 'Failed': 0.0, 'Skipped':...   \n",
       "Failed_tests                                             [None]   \n",
       "\n",
       "                                            external/kafka-0-10  \\\n",
       "attributes    {'Succeeded': 15.0, 'Failed': 0.0, 'Skipped': ...   \n",
       "Failed_tests                                             [None]   \n",
       "\n",
       "                                                   sql/catalyst  \\\n",
       "attributes    {'Succeeded': 2611.0, 'Failed': 1.0, 'Skipped'...   \n",
       "Failed_tests   [- daysToMillis and millisToDays *** FAILED ***]   \n",
       "\n",
       "                                                       sql/core  \\\n",
       "attributes    {'Succeeded': 4109.0, 'Failed': 1.0, 'Skipped'...   \n",
       "Failed_tests      [- SPARK-6330 regression test *** FAILED ***]   \n",
       "\n",
       "                                                       sql/hive  \\\n",
       "attributes    {'Succeeded': 2344.0, 'Failed': 510.0, 'Skippe...   \n",
       "Failed_tests  [- SPARK-22745 - read Hive's statistics for pa...   \n",
       "\n",
       "                                          sql/hive-thriftserver  \n",
       "attributes    {'Succeeded': 40.0, 'Failed': 0.0, 'Skipped': ...  \n",
       "Failed_tests                                             [None]  \n",
       "\n",
       "[2 rows x 194 columns]"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_json('results-10.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Test_group</th>\n",
       "      <th>Succeeded</th>\n",
       "      <th>Failed</th>\n",
       "      <th>Skipped</th>\n",
       "      <th>Pending</th>\n",
       "      <th>Aborted_tests</th>\n",
       "      <th>Failed_test</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>535</th>\n",
       "      <td>tools</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>*** RUN ABORTED ***</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Test_group  Succeeded  Failed  Skipped  Pending        Aborted_tests  \\\n",
       "535      tools        NaN     NaN      NaN      NaN  *** RUN ABORTED ***   \n",
       "\n",
       "    Failed_test  \n",
       "535         NaN  "
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read the scala-end-results.txt file if it has some input in it\n",
    "if os.path.getsize('scala-end-results.txt') > 1:\n",
    "    # Read the scala-end-results.txt file\n",
    "    df1 = pd.read_csv('scala-end-results.txt', header=None, sep='.txt:', engine='python')\n",
    "    # Give column names\n",
    "    df1.columns = ['Test_group','Results']\n",
    "    # Iliminate the path to SparkTestSuite file and keep only the module name \n",
    "    df1['Test_group'] = df1['Test_group'].str.replace(\"/target/surefire-reports/SparkTestSuite\",\"\")\n",
    "    # Iliminate all strings in Results column\n",
    "    df1['Results'] = df1['Results'].str.replace(\"Tests: succeeded\",\"\").str.replace(\"failed\",\"\").str.replace(\"canceled\",\"\").str.replace(\"ignored\",\"\").str.replace(\"pending\",\"\")\n",
    "    # Split the Results column into sevral columns\n",
    "    splitted_columns = df1['Results'].str.split(',', expand=True)\n",
    "    # Give column names\n",
    "    splitted_columns.columns = ['Succeeded', 'Failed', 'Canceled', 'Ignored', 'Pending']\n",
    "    # Transform string to integers\n",
    "    splitted_columns = splitted_columns.astype(int)\n",
    "    # Concatinate new df\n",
    "    df1 = pd.concat([df1, splitted_columns], axis = 1).drop(columns = ['Results'])\n",
    "    # Join the columns Canceled and Ignored in one signle coulumn\n",
    "    df1['Skipped'] = df1['Canceled'] + df1['Ignored']\n",
    "    # Drop the columns which are not needed anymore\n",
    "    df1 = df1.drop(columns = ['Canceled', 'Ignored'])\n",
    "    # Reorder the columns\n",
    "    df1 = df1[['Test_group', 'Succeeded', 'Failed', 'Skipped', 'Pending']]\n",
    "else:\n",
    "    # Exit since no test has run\n",
    "    columns = ['Test_group', 'Succeeded', 'Failed', 'Skipped', 'Pending']\n",
    "    df1 = pd.DataFrame(columns = columns)\n",
    "    print(\"No module has run\")\n",
    "\n",
    "# Read the aborted-tests.txt file if it has some input in it\n",
    "if os.path.getsize('aborted-tests.txt') > 1:\n",
    "    df2 = pd.read_csv('aborted-tests.txt', header=None, sep='.txt:', engine='python')\n",
    "    # Give column names\n",
    "    df2.columns = ['Test_group','Aborted_tests']\n",
    "    # Iliminate the path to SparkTestSuite file and keep only the module name \n",
    "    df2['Test_group'] = df2['Test_group'].str.replace(\"/target/surefire-reports/SparkTestSuite\",\"\")\n",
    "    # Merge df1 and df2\n",
    "    df1 = pd.concat([df1, df2] , ignore_index=True)\n",
    "else:\n",
    "    # Give the dataframe the column Aborted_tests with empty values\n",
    "    df1[\"Aborted_tests\"] = None\n",
    "    print(\"No aborted tests\")\n",
    "\n",
    "# Read the scala-tests.txt file if it has some input in it\n",
    "if os.path.getsize('scala-tests.txt') > 1:\n",
    "    df3 = pd.read_csv('scala-tests.txt', header=None, sep='.txt:', engine='python')\n",
    "    # Give column names\n",
    "    df3.columns = ['Test_group','Failed_test']\n",
    "    # Iliminate the path to SparkTestSuite file and keep only the module name \n",
    "    df3['Test_group'] = df3['Test_group'].str.replace(\"/target/surefire-reports/SparkTestSuite\",\"\")\n",
    "    # Merge new df1 and df3\n",
    "    df1 = pd.merge(df1, df3, how='outer')\n",
    "else:\n",
    "    # Give the dataframe the column Failed_test with empty colmun\n",
    "    df1[\"Failed_test\"] = None\n",
    "    print(\"No scala test errors\")\n",
    "\n",
    "df1[df1['Test_group']=='tools']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Test_group</th>\n",
       "      <th>Failed_test</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>org.apache.hadoop.util.TestReadWriteDiskValidator</td>\n",
       "      <td>testReadWriteDiskValidator</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>org.apache.hadoop.util.TestReadWriteDiskValidator</td>\n",
       "      <td>testCheckFailures</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>org.apache.hadoop.io.compress.TestCompressorDe...</td>\n",
       "      <td>testCompressorDecompressor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>org.apache.hadoop.io.compress.TestCompressorDe...</td>\n",
       "      <td>testCompressorDecompressorWithExeedBufferLimit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>org.apache.hadoop.io.compress.snappy.TestSnapp...</td>\n",
       "      <td>testSnappyCompressDecompressInMultiThreads</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>org.apache.hadoop.io.compress.snappy.TestSnapp...</td>\n",
       "      <td>testSnappyCompressDecompress</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>org.apache.hadoop.security.TestLdapGroupsMapping</td>\n",
       "      <td>testLdapReadTimeout</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>org.apache.hadoop.security.TestLdapGroupsMapping</td>\n",
       "      <td>testLdapConnectionTimeout</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>org.apache.hadoop.hdfs.TestFileChecksum</td>\n",
       "      <td>testStripedFileChecksumWithMissedDataBlocksRan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>org.apache.hadoop.hdfs.TestFileChecksum</td>\n",
       "      <td>testStripedFileChecksumWithMissedDataBlocksRan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>org.apache.hadoop.hdfs.TestFileChecksum</td>\n",
       "      <td>testStripedFileChecksumWithMissedDataBlocksRan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>org.apache.hadoop.hdfs.TestFileChecksum</td>\n",
       "      <td>testStripedFileChecksumWithMissedDataBlocksRan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>org.apache.hadoop.hdfs.TestFileChecksum</td>\n",
       "      <td>testStripedFileChecksumWithMissedDataBlocksRan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>org.apache.hadoop.hdfs.TestFileChecksum</td>\n",
       "      <td>testStripedFileChecksumWithMissedDataBlocksRan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>org.apache.hadoop.hdfs.TestFileChecksum</td>\n",
       "      <td>testStripedFileChecksumWithMissedDataBlocksRan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>org.apache.hadoop.hdfs.TestFileChecksum</td>\n",
       "      <td>testStripedFileChecksumWithMissedDataBlocks1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>org.apache.hadoop.hdfs.TestFileChecksum</td>\n",
       "      <td>testStripedFileChecksumWithMissedDataBlocks2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>org.apache.hadoop.hdfs.TestFileChecksum</td>\n",
       "      <td>testStripedFileChecksumWithMissedDataBlocksRan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>org.apache.hadoop.hdfs.TestFileChecksum</td>\n",
       "      <td>testStripedFileChecksumWithMissedDataBlocksRan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>org.apache.hadoop.hdfs.TestFileChecksumComposi...</td>\n",
       "      <td>testStripedFileChecksumWithMissedDataBlocksRan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>org.apache.hadoop.hdfs.TestFileChecksumComposi...</td>\n",
       "      <td>testStripedFileChecksumWithMissedDataBlocksRan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>org.apache.hadoop.hdfs.TestFileChecksumComposi...</td>\n",
       "      <td>testStripedFileChecksumWithMissedDataBlocksRan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>org.apache.hadoop.hdfs.TestFileChecksumComposi...</td>\n",
       "      <td>testStripedFileChecksumWithMissedDataBlocks1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>org.apache.hadoop.hdfs.TestFileChecksumComposi...</td>\n",
       "      <td>testStripedFileChecksumWithMissedDataBlocks2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>org.apache.hadoop.hdfs.TestFileChecksumComposi...</td>\n",
       "      <td>testStripedFileChecksumWithMissedDataBlocksRan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>org.apache.hadoop.hdfs.TestReconstructStripedF...</td>\n",
       "      <td>testRecoverAnyBlocks1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>org.apache.hadoop.hdfs.TestReconstructStripedF...</td>\n",
       "      <td>testRecoverOneDataBlock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>org.apache.hadoop.hdfs.TestReconstructStripedF...</td>\n",
       "      <td>testRecoverOneParityBlock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>org.apache.hadoop.mapred.TestMRTimelineEventHa...</td>\n",
       "      <td>testMRNewTimelineServiceEventHandling</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>nodeFile null</td>\n",
       "      <td>testSimulatorRunning[Testing with: SYNTH, org....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           Test_group  \\\n",
       "0   org.apache.hadoop.util.TestReadWriteDiskValidator   \n",
       "1   org.apache.hadoop.util.TestReadWriteDiskValidator   \n",
       "8   org.apache.hadoop.io.compress.TestCompressorDe...   \n",
       "9   org.apache.hadoop.io.compress.TestCompressorDe...   \n",
       "16  org.apache.hadoop.io.compress.snappy.TestSnapp...   \n",
       "17  org.apache.hadoop.io.compress.snappy.TestSnapp...   \n",
       "24   org.apache.hadoop.security.TestLdapGroupsMapping   \n",
       "25   org.apache.hadoop.security.TestLdapGroupsMapping   \n",
       "32            org.apache.hadoop.hdfs.TestFileChecksum   \n",
       "33            org.apache.hadoop.hdfs.TestFileChecksum   \n",
       "34            org.apache.hadoop.hdfs.TestFileChecksum   \n",
       "35            org.apache.hadoop.hdfs.TestFileChecksum   \n",
       "36            org.apache.hadoop.hdfs.TestFileChecksum   \n",
       "37            org.apache.hadoop.hdfs.TestFileChecksum   \n",
       "38            org.apache.hadoop.hdfs.TestFileChecksum   \n",
       "39            org.apache.hadoop.hdfs.TestFileChecksum   \n",
       "40            org.apache.hadoop.hdfs.TestFileChecksum   \n",
       "41            org.apache.hadoop.hdfs.TestFileChecksum   \n",
       "42            org.apache.hadoop.hdfs.TestFileChecksum   \n",
       "64  org.apache.hadoop.hdfs.TestFileChecksumComposi...   \n",
       "65  org.apache.hadoop.hdfs.TestFileChecksumComposi...   \n",
       "66  org.apache.hadoop.hdfs.TestFileChecksumComposi...   \n",
       "67  org.apache.hadoop.hdfs.TestFileChecksumComposi...   \n",
       "68  org.apache.hadoop.hdfs.TestFileChecksumComposi...   \n",
       "69  org.apache.hadoop.hdfs.TestFileChecksumComposi...   \n",
       "85  org.apache.hadoop.hdfs.TestReconstructStripedF...   \n",
       "86  org.apache.hadoop.hdfs.TestReconstructStripedF...   \n",
       "87  org.apache.hadoop.hdfs.TestReconstructStripedF...   \n",
       "91  org.apache.hadoop.mapred.TestMRTimelineEventHa...   \n",
       "95                                      nodeFile null   \n",
       "\n",
       "                                          Failed_test  \n",
       "0                          testReadWriteDiskValidator  \n",
       "1                                   testCheckFailures  \n",
       "8                          testCompressorDecompressor  \n",
       "9      testCompressorDecompressorWithExeedBufferLimit  \n",
       "16         testSnappyCompressDecompressInMultiThreads  \n",
       "17                       testSnappyCompressDecompress  \n",
       "24                                testLdapReadTimeout  \n",
       "25                          testLdapConnectionTimeout  \n",
       "32  testStripedFileChecksumWithMissedDataBlocksRan...  \n",
       "33  testStripedFileChecksumWithMissedDataBlocksRan...  \n",
       "34  testStripedFileChecksumWithMissedDataBlocksRan...  \n",
       "35  testStripedFileChecksumWithMissedDataBlocksRan...  \n",
       "36  testStripedFileChecksumWithMissedDataBlocksRan...  \n",
       "37  testStripedFileChecksumWithMissedDataBlocksRan...  \n",
       "38  testStripedFileChecksumWithMissedDataBlocksRan...  \n",
       "39       testStripedFileChecksumWithMissedDataBlocks1  \n",
       "40       testStripedFileChecksumWithMissedDataBlocks2  \n",
       "41  testStripedFileChecksumWithMissedDataBlocksRan...  \n",
       "42  testStripedFileChecksumWithMissedDataBlocksRan...  \n",
       "64  testStripedFileChecksumWithMissedDataBlocksRan...  \n",
       "65  testStripedFileChecksumWithMissedDataBlocksRan...  \n",
       "66  testStripedFileChecksumWithMissedDataBlocksRan...  \n",
       "67       testStripedFileChecksumWithMissedDataBlocks1  \n",
       "68       testStripedFileChecksumWithMissedDataBlocks2  \n",
       "69  testStripedFileChecksumWithMissedDataBlocksRan...  \n",
       "85                              testRecoverAnyBlocks1  \n",
       "86                            testRecoverOneDataBlock  \n",
       "87                          testRecoverOneParityBlock  \n",
       "91              testMRNewTimelineServiceEventHandling  \n",
       "95  testSimulatorRunning[Testing with: SYNTH, org....  "
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read output-tests.csv file as dataframe\n",
    "df = pd.read_csv('output-tests.csv')\n",
    "# Give column names\n",
    "df.columns = ['Tests_run', 'Failures', 'Errors', 'Skipped', 'Test_group']\n",
    "# Erase all unnecessary characters in each column and transform them into type int64\n",
    "df['Tests_run'] = df['Tests_run'].str.split(':', n=1).str[1].astype('int64')\n",
    "df['Failures'] = df['Failures'].str.split(':', n=1).str[1].astype('int64')\n",
    "df['Errors'] = df['Errors'].str.split(':', n=1).str[1].astype('int64')\n",
    "df['Skipped'] = df['Skipped'].str.split(':', n=1).str[1].astype('int64')\n",
    "df['Test_group'] = df['Test_group'].str.split('-', n=1).str[1].str.replace('in','')\n",
    "# delete lines where no testname is given which corresponds to the total columns\n",
    "df = df.dropna(subset=['Test_group'])\n",
    "# Add empty columns below to be compatible with schema\n",
    "df['Pending'] = [None] * len(df)\n",
    "df['Aborted_tests'] = [None] * len(df)\n",
    "df['Failed']= df['Failures'] + df['Errors']\n",
    "# Succeeded is the nuber of test runs minus failed and skipped test\n",
    "df['Succeeded'] = df['Tests_run'] - df['Failed'] - df['Skipped']\n",
    "# Select and reorder the columns\n",
    "df= df[['Test_group', 'Succeeded', 'Failed', 'Skipped', 'Pending', 'Aborted_tests']]\n",
    "\n",
    "# Read the java-test-failures.txt file if it has some input in it\n",
    "if os.path.getsize('java-test-failures.txt') > 1:\n",
    "    # Read output-tests.csv file as dataframe\n",
    "    df2 = pd.read_csv('java-test-failures.txt', header = None, delimiter='/t', engine='python')\n",
    "    # Name the single column Tests\n",
    "    df2.columns = ['Tests']\n",
    "    # Split the columns into two coulumns named Failed_test and Test_group\n",
    "    df2[['Failed_test', 'Test_group']] = df2['Tests'].str.split('(', n=1, expand= True)\n",
    "    # Remove the column Tests\n",
    "    df2.drop(columns=['Tests'], inplace= True)\n",
    "    # Split the columns Test_group into two after the )\n",
    "    df2[['Test_group', 'unnecsessary']] = df2['Test_group'].str.split(')', n=1, expand= True)\n",
    "    # Remove evrything after the parnetheses\n",
    "    df2.drop(columns=['unnecsessary'], inplace= True)\n",
    "    # Reorder columns\n",
    "    df2= df2[['Test_group', 'Failed_test']]\n",
    "    df = pd.merge(df, df2, on = 'Test_group', how='outer')\n",
    "else:\n",
    "    # Create the column Failed_test with empty values\n",
    "    df['Failed_test'] = None\n",
    "    print(\"No java failed tests\")\n",
    "\n",
    "df2 = df2.drop_duplicates()\n",
    "df2 = df2.drop_duplicates()\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tests_run</th>\n",
       "      <th>Failures</th>\n",
       "      <th>Errors</th>\n",
       "      <th>Skipped</th>\n",
       "      <th>Test_group</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>org.apache.spark.util.kvstore.InMemoryStoreS...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>org.apache.spark.util.kvstore.ArrayWrappersS...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>38</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>org.apache.spark.util.kvstore.LevelDBIterato...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>38</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>org.apache.spark.util.kvstore.InMemoryIterat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>org.apache.spark.util.kvstore.LevelDBTypeInf...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>org.apache.spark.streamg.kafka010.JavaLocati...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>org.apache.spark.streamg.kafka010.JavaDirect...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>org.apache.spark.streamg.kafka010.JavaKafkaR...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>org.apache.spark.streamg.flume.JavaFlumeStre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>org.apache.spark.streamg.flume.JavaFlumePoll...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>158 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Tests_run  Failures  Errors  Skipped  \\\n",
       "2            6         0       0        0   \n",
       "3            1         0       0        0   \n",
       "4           38         0       0        0   \n",
       "5           38         0       0        0   \n",
       "6           10         0       0        0   \n",
       "..         ...       ...     ...      ...   \n",
       "171          1         0       0        0   \n",
       "172          1         0       0        0   \n",
       "173          1         0       0        0   \n",
       "177          1         0       0        0   \n",
       "178          1         0       0        0   \n",
       "\n",
       "                                            Test_group  \n",
       "2      org.apache.spark.util.kvstore.InMemoryStoreS...  \n",
       "3      org.apache.spark.util.kvstore.ArrayWrappersS...  \n",
       "4      org.apache.spark.util.kvstore.LevelDBIterato...  \n",
       "5      org.apache.spark.util.kvstore.InMemoryIterat...  \n",
       "6      org.apache.spark.util.kvstore.LevelDBTypeInf...  \n",
       "..                                                 ...  \n",
       "171    org.apache.spark.streamg.kafka010.JavaLocati...  \n",
       "172    org.apache.spark.streamg.kafka010.JavaDirect...  \n",
       "173    org.apache.spark.streamg.kafka010.JavaKafkaR...  \n",
       "177    org.apache.spark.streamg.flume.JavaFlumeStre...  \n",
       "178    org.apache.spark.streamg.flume.JavaFlumePoll...  \n",
       "\n",
       "[158 rows x 5 columns]"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('output-tests.csv')\n",
    "# Give column names\n",
    "df.columns = ['Tests_run', 'Failures', 'Errors', 'Skipped', 'Test_group']\n",
    "# Erase all unnecessary characters in each column and transform them into type int64\n",
    "df['Tests_run'] = df['Tests_run'].str.split(':', n=1).str[1].astype('int64')\n",
    "df['Failures'] = df['Failures'].str.split(':', n=1).str[1].astype('int64')\n",
    "df['Errors'] = df['Errors'].str.split(':', n=1).str[1].astype('int64')\n",
    "df['Skipped'] = df['Skipped'].str.split(':', n=1).str[1].astype('int64')\n",
    "df['Test_group'] = df['Test_group'].str.split('-', n=1).str[1].str.replace('in','')\n",
    "# delete lines where no testname is given which corresponds to the total columns\n",
    "df = df.dropna(subset=['Test_group'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.read_csv('java-test-failures.txt', header = None, delimiter='/t', engine='python')\n",
    "# Name the single column Tests\n",
    "df2.columns = ['Tests']\n",
    "# Split the columns into two coulumns named Failed_test and Test_group\n",
    "df2[['Failed_test', 'Test_group']] = df2['Tests'].str.split('(', n=1, expand= True)\n",
    "# Remove the column Tests\n",
    "df2.drop(columns=['Tests'], inplace= True)\n",
    "# Split the columns Test_group into two after the )\n",
    "df2[['Test_group', 'unnecsessary']] = df2['Test_group'].str.split(')', n=1, expand= True)\n",
    "# Remove evrything after the parnetheses\n",
    "df2.drop(columns=['unnecsessary'], inplace= True)\n",
    "# Reorder columns\n",
    "df2= df2[['Test_group', 'Failed_test']]\n",
    "df2.drop_duplicates()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Test_group</th>\n",
       "      <th>Aborted_tests</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tools</td>\n",
       "      <td>*** RUN ABORTED ***</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Test_group        Aborted_tests\n",
       "0      tools  *** RUN ABORTED ***"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2 = pd.read_csv('aborted-tests.txt', header=None, sep='.txt:', engine='python')\n",
    "# Give column names\n",
    "df2.columns = ['Test_group','Aborted_tests']\n",
    "# Iliminate the path to SparkTestSuite file and keep only the module name \n",
    "df2['Test_group'] = df2['Test_group'].str.replace(\"/target/surefire-reports/SparkTestSuite\",\"\")\n",
    "# Merge df1 and df2\n",
    "df2\n",
    "#df1 = pd.concat([df1, df2] , ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                            Test_group        Aborted_tests  \\\n",
      "0      org.apache.spark.util.kvstore.InMemoryStoreS...                  NaN   \n",
      "1      org.apache.spark.util.kvstore.ArrayWrappersS...                  NaN   \n",
      "2      org.apache.spark.util.kvstore.LevelDBIterato...                  NaN   \n",
      "3      org.apache.spark.util.kvstore.InMemoryIterat...                  NaN   \n",
      "4      org.apache.spark.util.kvstore.LevelDBTypeInf...                  NaN   \n",
      "..                                                 ...                  ...   \n",
      "719                                           sql/hive                  NaN   \n",
      "720                                           sql/hive                  NaN   \n",
      "721                                           sql/hive                  NaN   \n",
      "722                              sql/hive-thriftserver                  NaN   \n",
      "723                                              tools  *** RUN ABORTED ***   \n",
      "\n",
      "                                           Failed_test  \n",
      "0                                                  NaN  \n",
      "1                                                  NaN  \n",
      "2                                                  NaN  \n",
      "3                                                  NaN  \n",
      "4                                                  NaN  \n",
      "..                                                 ...  \n",
      "719  - SPARK-8020: set sql conf in spark conf *** F...  \n",
      "720  - SPARK-9757 Persist Parquet relation with dec...  \n",
      "721  - SPARK-16901: set javax.jdo.option.Connection...  \n",
      "722                                                NaN  \n",
      "723                                                NaN  \n",
      "\n",
      "[724 rows x 3 columns]\n",
      "                                            Test_group  Aborted_tests  \\\n",
      "0      org.apache.spark.util.kvstore.InMemoryStoreS...            NaN   \n",
      "1      org.apache.spark.util.kvstore.ArrayWrappersS...            NaN   \n",
      "2      org.apache.spark.util.kvstore.LevelDBIterato...            NaN   \n",
      "3      org.apache.spark.util.kvstore.InMemoryIterat...            NaN   \n",
      "4      org.apache.spark.util.kvstore.LevelDBTypeInf...            NaN   \n",
      "..                                                 ...            ...   \n",
      "719                                           sql/hive            NaN   \n",
      "720                                           sql/hive            NaN   \n",
      "721                                           sql/hive            NaN   \n",
      "722                              sql/hive-thriftserver            NaN   \n",
      "723                                              tools            NaN   \n",
      "\n",
      "                                           Failed_test  \n",
      "0                                                  NaN  \n",
      "1                                                  NaN  \n",
      "2                                                  NaN  \n",
      "3                                                  NaN  \n",
      "4                                                  NaN  \n",
      "..                                                 ...  \n",
      "719  - SPARK-8020: set sql conf in spark conf *** F...  \n",
      "720  - SPARK-9757 Persist Parquet relation with dec...  \n",
      "721  - SPARK-16901: set javax.jdo.option.Connection...  \n",
      "722                                                NaN  \n",
      "723                                                NaN  \n",
      "\n",
      "[724 rows x 3 columns]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "numpy.float64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "with open(f'results-1.json', 'r') as json_file:\n",
    "    json_data = json.load(json_file)\n",
    "\n",
    "# Convert the nested dictionary into a DataFrame\n",
    "# Create a list called record\n",
    "    records = []\n",
    "    # Loop through each element of the json file\n",
    "    for test_group, values in json_data.items():\n",
    "        # For each element in the failed_test list\n",
    "        for failed_test in values['Failed_tests']:\n",
    "            # Get the class Test_group and define it as a variable test_group\n",
    "            record = {'Test_group': test_group}\n",
    "            # Get the attribute values\n",
    "            record.update(values['attributes'])\n",
    "            # Get the list of failed tests\n",
    "            record['Failed_test'] = failed_test\n",
    "            # Append all these elements to the list records\n",
    "            records.append(record)\n",
    "\n",
    "# Define the dataframe\n",
    "df = pd.DataFrame(records)\n",
    "df.drop(columns= ['Succeeded', 'Failed', 'Skipped', 'Pending'], inplace=True)\n",
    "\n",
    "# Read the JSON data as a dictionary\n",
    "with open('results-10.json', 'r') as json_file:\n",
    "    json_data = json.load(json_file)\n",
    "\n",
    "# Convert the nested dictionary into a DataFrame\n",
    "# Create a list called record\n",
    "records = []\n",
    "# Loop through each element of the json file\n",
    "for test_group, values in json_data.items():\n",
    "    # For each element in the failed_test list\n",
    "    for failed_test in values['Failed_tests']:\n",
    "        # Get the class Test_group and define it as a variable test_group\n",
    "        record = {'Test_group': test_group}\n",
    "        # Get the attribute values\n",
    "        record.update(values['attributes'])\n",
    "        # Get the list of failed tests\n",
    "        record['Failed_test'] = failed_test\n",
    "        # Append all these elements to the list records\n",
    "        records.append(record)\n",
    "\n",
    "# Define the dataframe\n",
    "df_external = pd.DataFrame(records)\n",
    "# Drop columns that we do not compare\n",
    "df_external.drop(columns= ['Succeeded', 'Failed', 'Skipped', 'Pending'], inplace=True)\n",
    "\n",
    "\n",
    "print(df)\n",
    "print(df_external)\n",
    "#df = df.merge(df_external, how='left', indicator=True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
